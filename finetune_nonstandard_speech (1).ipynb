{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21c1499",
   "metadata": {},
   "source": [
    "# Fine-tune Whisper on CDLI Non-Standard Speech Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43222608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 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\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "HF_TOKEN = input()\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f87c76",
   "metadata": {},
   "source": [
    "## Settings \n",
    "\n",
    "--> adapt for your scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3ce04",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95aac5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will write model to: /jupyter_kernel/trained_models/en_nonstandard_tune_whisper_small_2\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# storage in Volume that will persist\n",
    "LOCAL_STORAGE_DIR = '/jupyter_kernel'\n",
    "\n",
    "BASE_DIR = os.path.join(LOCAL_STORAGE_DIR, 'trained_models')\n",
    "!mkdir -p {BASE_DIR}\n",
    "\n",
    "# directory for model training\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'en_nonstandard_tune_whisper_small_2')\n",
    "#OUTPUT_DIR = os.path.join(BASE_DIR, 'en_nonstandard_tune_whisper_tiny_1')\n",
    "\n",
    "print(f\"Will write model to: {OUTPUT_DIR}\")\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    raise ValueError(f\"Output directory already exists - if you continue this will overwrite data and may lead to strange results...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341cb64",
   "metadata": {},
   "source": [
    "### Model and Dataset settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0091bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#WHISPER_MODEL_TYPE = \"openai/whisper-tiny\" \n",
    "WHISPER_MODEL_TYPE = \"openai/whisper-small\" \n",
    "# WHISPER_MODEL_TYPE = \"openai/whisper-large-v3\" \n",
    "\n",
    "LANGUAGE = 'en'\n",
    "DATASET_NAME = \"cdli/kenyan_english_nonstandard_speech_v0.9\"\n",
    "\n",
    "# LANGUAGE = 'sw'\n",
    "#DATASET_NAME = \"cdli/kenyan_swahili_nonstandard_speech_v0.9\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c808490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which parts of the model to update\n",
    "UPDATE_ENCODER = True\n",
    "UPDATE_PROJ = True\n",
    "UPDATE_DECODER = False\n",
    "\n",
    "# Turn on SpecAugment\n",
    "USE_SPECAUGMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf2f146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model will be loaded from: openai/whisper-small\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######################\n",
    "## don't change these!\n",
    "######################\n",
    "\n",
    "\n",
    "TASK = \"transcribe\"\n",
    "\n",
    "BASE_MODEL_NAME = WHISPER_MODEL_TYPE\n",
    "print('Base model will be loaded from:', BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2735de",
   "metadata": {},
   "source": [
    "### Trainer Settings\n",
    "\n",
    "--> adjust as needed or keep defaults (these settings should be a good starting point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "104baa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING_STEPS = 10  # Increased slightly since training will be longer\n",
    "# if save steps is 0, only last and best model will be written\n",
    "SAVE_STEPS = 100    # Increased to reduce checkpoint frequency\n",
    "\n",
    "# training duration\n",
    "MAX_EPOCHS = 8      # Reduced from 10 for more reasonable training time\n",
    "MAX_STEPS = 600     # Reduced from 1000 - sufficient for small model\n",
    "\n",
    "# Learning Rate and LR Scheduler (LR_END and LR_DECAY_POWER only apply to polynomial)\n",
    "LEARNING_RATE = 1e-4 #@param - Good for small model\n",
    "LR_SCHEDULER_TYPE = 'polynomial' # constant_with_warmup or polynomial\n",
    "LR_WARMUP_STEPS = 50\n",
    "LR_END = 1e-8\n",
    "LR_DECAY_POWER = 4\n",
    "\n",
    "BATCH_SIZE = 12     # Reduced from 32 - small model needs smaller batches\n",
    "EVAL_BATCH_SIZE = 8 # Reduced from 16\n",
    "\n",
    "#@markdown other settings relevant for evaluation\n",
    "MAX_GEN_LEN = 128 # increase if your data has long sequences!\n",
    "EVAL_ON_START = True\n",
    "EVAL_STEPS = 50    # Good frequency for monitoring\n",
    "\n",
    "# for CPU, set both to false\n",
    "USE_FP16 = True    # Keep enabled for memory efficiency\n",
    "USE_BF16 = False   # only some GPUs support this, eg A100, A40\n",
    "\n",
    "# checkpoints get huge for large models (~18 GB!)\n",
    "NUM_CHECKPOINTS_TO_STORE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57078735",
   "metadata": {},
   "source": [
    "## Imports and Prep\n",
    "\n",
    "--> no need to change anything here, just run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "effd0080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: False\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# more efficient dataset handling\n",
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()\n",
    "\n",
    "# check if we have gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7034f947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84f49973bab47128779dd9eccbc95f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fccc4bda53145bcad42b2d5de20a1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import random\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "\n",
    "import tarfile\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import evaluate\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "transcript_normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "903eac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "290a30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wer(references, predictions, normalize=True, verbose=True):\n",
    "  rs = references\n",
    "  ps = predictions\n",
    "  if normalize:\n",
    "    ps = [transcript_normalizer(x) for x in predictions]\n",
    "    rs = [transcript_normalizer(x) for x in references]\n",
    "  if verbose:\n",
    "    for r, p in zip(rs, ps):\n",
    "      print(r)\n",
    "      print(p)\n",
    "      print()\n",
    "\n",
    "  return wer_metric.compute(references=rs, predictions=ps)\n",
    "\n",
    "def compute_lattescore(references, predictions, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    LATTEScore calculation - percentage of transcripts that preserve meaning\n",
    "    Based on the paper: Large Language Models As A Proxy For Human Evaluation\n",
    "    This is a simplified version using semantic similarity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to use sentence transformers for better semantic similarity\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        preserved_count = 0\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            # Skip empty strings\n",
    "            if not ref.strip() or not pred.strip():\n",
    "                continue\n",
    "                \n",
    "            # Get sentence embeddings\n",
    "            emb_ref = model.encode(ref, convert_to_tensor=True)\n",
    "            emb_pred = model.encode(pred, convert_to_tensor=True)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = util.pytorch_cos_sim(emb_ref, emb_pred).item()\n",
    "            \n",
    "            # Consider meaning preserved if similarity > threshold\n",
    "            if similarity > similarity_threshold:\n",
    "                preserved_count += 1\n",
    "        \n",
    "        total_count = len([r for r in references if r.strip()])\n",
    "        lattescore = (preserved_count / total_count) * 100 if total_count > 0 else 0\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: use WER-based approximation if sentence-transformers not available\n",
    "        print(\"Sentence transformers not available, using WER-based LATTEScore approximation\")\n",
    "        preserved_count = 0\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            # Skip empty strings\n",
    "            if not ref.strip() or not pred.strip():\n",
    "                continue\n",
    "                \n",
    "            wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "            # Conservative threshold: meaning preserved if WER < 0.3 (30%)\n",
    "            if wer < 0.3:\n",
    "                preserved_count += 1\n",
    "        \n",
    "        total_count = len([r for r in references if r.strip()])\n",
    "        lattescore = (preserved_count / total_count) * 100 if total_count > 0 else 0\n",
    "    \n",
    "    return lattescore\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for ASR evaluation including WER, CER, and LATTEScore\n",
    "    \"\"\"\n",
    "    # for training metrics\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_strs = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_strs = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # calculate a per-example average for WER and CER\n",
    "    wers = []\n",
    "    cers = []\n",
    "    for pred_str, label_str in zip(pred_strs, label_strs):\n",
    "        p = transcript_normalizer(pred_str)\n",
    "        l = transcript_normalizer(label_str)\n",
    "        # Skip empty strings for metric calculation\n",
    "        if l.strip() and p.strip():\n",
    "            wer = wer_metric.compute(predictions=[p], references=[l])\n",
    "            cer = cer_metric.compute(predictions=[p], references=[l])\n",
    "            wers.append(wer)\n",
    "            cers.append(cer)\n",
    "\n",
    "    wer = np.mean([min(1.0, x) for x in wers]) if wers else 1.0\n",
    "    cer = np.mean([min(1.0, x) for x in cers]) if cers else 1.0\n",
    "    \n",
    "    # Calculate LATTEScore\n",
    "    lattescore = compute_lattescore(label_strs, pred_strs)\n",
    "    \n",
    "    print('=== Metrics ===')\n",
    "    print(f'Adjusted WER: {wer:.4f}')\n",
    "    print(f'Adjusted CER: {cer:.4f}')\n",
    "    print(f'LATTEScore: {lattescore:.2f}%')\n",
    "    print(f'Un-adjusted WER: {np.mean(wers) if wers else 1.0:.4f}')\n",
    "    print(f'Un-adjusted CER: {np.mean(cers) if cers else 1.0:.4f}')\n",
    "    print('===============')\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer, \n",
    "        \"cer\": cer, \n",
    "        \"lattescore\": lattescore\n",
    "    }\n",
    "\n",
    "# Optional: Add a function to analyze model quality based on LATTEScore\n",
    "def analyze_model_deployment(lattescore, threshold=80.0):\n",
    "    \"\"\"\n",
    "    Analyze if model meets quality standards for deployment based on LATTEScore\n",
    "    Using the 80% threshold from the research paper\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Model Deployment Analysis ===\")\n",
    "    print(f\"LATTEScore: {lattescore:.2f}%\")\n",
    "    print(f\"Deployment Threshold: {threshold}%\")\n",
    "    \n",
    "    if lattescore >= threshold:\n",
    "        print(\"✅ RECOMMENDATION: Model meets quality standards for deployment\")\n",
    "        print(\"   The ASR model preserves meaning in most transcripts\")\n",
    "    else:\n",
    "        print(\"❌ RECOMMENDATION: Model does not meet quality standards\")\n",
    "        print(\"   Consider: More training data, hyperparameter tuning, or different architecture\")\n",
    "    \n",
    "    return lattescore >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "105389f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name, split='test', limit_to_30_seconds=True):\n",
    "    \"\"\"\n",
    "    Load a dataset from Hugging Face Hub.\n",
    "    If limit_to_30_seconds is True, will only load examples with audio length <= 30 seconds.\n",
    "    \"\"\"\n",
    "    if split not in ['train', 'test', 'validation']:\n",
    "        raise ValueError(\"split must be one of 'train', 'test', or 'validation'\")\n",
    "    ds = datasets.load_dataset(dataset_name, split=split, streaming=False)\n",
    "    orig_len = len(ds)\n",
    "    if limit_to_30_seconds:\n",
    "        ds = ds.filter(lambda example: example['audio_length'] <= 30)\n",
    "        print(f\"Filtered dataset from {orig_len} to {len(ds)} examples with audio length <= 30 seconds\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac36c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following warning can be ignored:\n",
    "# \"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
    "# See: https://discuss.huggingface.co/t/finetuning-whisper-attention-mask-not-set-and-canot-be-inferred/97456\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb8215",
   "metadata": {},
   "source": [
    "## Download datasets and prepare features\n",
    "\n",
    "--> no need to change anything here, just run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcf0e7",
   "metadata": {},
   "source": [
    "### Optimizing some settings for dataset access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c69a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: False\n",
      "device is:  cuda\n",
      "# processors: 24\n"
     ]
    }
   ],
   "source": [
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is: ', device)\n",
    "\n",
    "# IMPORTANT! need to set to 1 to avoid the mapping to hang!\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()\n",
    "\n",
    "num_proc = min(32, os.cpu_count())\n",
    "print('# processors:', num_proc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a27923",
   "metadata": {},
   "source": [
    "### Load feature extractor\n",
    "\n",
    "--> for the model type you specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85919d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Language:  en\n",
      "Using model: openai/whisper-small\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load processor\n",
    "print('Using Language: ', LANGUAGE)\n",
    "print('Using model:', WHISPER_MODEL_TYPE)\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_TYPE, language=LANGUAGE, task=TASK)\n",
    "\n",
    "# since this tokenizer isn't a FastTokenizer, so there is no point in running it with is_batched=True\n",
    "# see: processor.tokenizer.is_fast\n",
    "def prepare_features(example):\n",
    "    example[\"input_features\"] = processor.feature_extractor(example[\"audio\"][\"array\"], sampling_rate=example[\"audio\"][\"sampling_rate\"]).input_features[0]\n",
    "    example[\"labels\"] = processor.tokenizer(example[\"transcription\"]).input_ids\n",
    "    # also count number of tokens\n",
    "    example[\"token_length\"] = len(example[\"labels\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d19cd",
   "metadata": {},
   "source": [
    "### Load non-standard speech dataset\n",
    "\n",
    "We need to filter to 30 seconds, as Whisper can only train on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a020684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f37bacf67a24f978681da346dcf9173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 4236 to 3130 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acee6c92b404fa1927f6c96e6d2e748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/3130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TRAIN dataset with 3130 examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(DATASET_NAME, split='train', limit_to_30_seconds=True)\n",
    "train_dataset = train_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded TRAIN dataset with {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3a95a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04929016620e4ca2acc2c16d0a869101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 993 to 705 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5b091fdb504a51942fd48f089250a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TEST dataset with 705 examples\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(DATASET_NAME, split='test', limit_to_30_seconds=True)\n",
    "test_dataset = test_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded TEST dataset with {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c34fa18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4575ff9ade46acac267f794fd3d403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/572 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 572 to 342 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1943784972514e70b8a618d3ebe01605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DEV dataset with 342 examples\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = load_dataset(DATASET_NAME, split='validation', limit_to_30_seconds=True)\n",
    "dev_dataset = dev_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded DEV dataset with {len(dev_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d1182",
   "metadata": {},
   "source": [
    "## Prepare Trainer\n",
    "\n",
    "--> no need to change anything here, just run\n",
    "\n",
    "Whenever something is changed in the settings, you need to rerun this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b402d733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Language:  en\n",
      "Using model: openai/whisper-small\n",
      "language set to: en\n"
     ]
    }
   ],
   "source": [
    "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
    "_ = base_model.to(device)\n",
    "print('Using Language: ', LANGUAGE)\n",
    "print('Using model:', WHISPER_MODEL_TYPE)\n",
    "\n",
    "# ensure task and language for training\n",
    "base_model.generation_config.language = LANGUAGE\n",
    "base_model.generation_config.task = TASK\n",
    "base_model.generation_config.forced_decoder_ids = None\n",
    "base_model.config.forced_decoder_ids = None\n",
    "# to use gradient checkpointing\n",
    "base_model.config.use_cache = False\n",
    "print('language set to:', base_model.generation_config.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f46b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using specaugment: True\n",
      "Using audio augmentation: True\n",
      "Audio augmentation probability: 0.7\n",
      "Using specaugment: True\n",
      "=== Using BOTH augmentation types: SpecAugment (feature-level) + Audio (signal-level) ===\n",
      "Audio augmentation functions defined. Use prepare_train_features for training data and prepare_eval_features for test/dev data.\n"
     ]
    }
   ],
   "source": [
    "# Add SpecAugment\n",
    "if USE_SPECAUGMENT:\n",
    "    base_model.config.apply_spec_augment = USE_SPECAUGMENT\n",
    "\n",
    "    # Specaugment (use default settings, as per paper)\n",
    "    # time masking\n",
    "    base_model.config.mask_time_prob = 0.05\n",
    "    base_model.config.mask_time_length = 10\n",
    "    base_model.config.mask_time_min_masks = 2\n",
    "\n",
    "    # feature masking\n",
    "    base_model.config.mask_feature_prob = 0.05 # def: 0\n",
    "    base_model.config.mask_feature_length = 10\n",
    "    base_model.config.mask_feature_min_masks = 2 # def: 0\n",
    "\n",
    "print('Using specaugment:', base_model.config.apply_spec_augment)\n",
    "\n",
    "# Add Audio Augmentation functions\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "def apply_audio_augmentation(audio_array, sample_rate=16000, augmentation_prob=0.7):\n",
    "    \"\"\"\n",
    "    Apply audio augmentations to training data\n",
    "    augmentation_prob: probability of applying any augmentation to an example\n",
    "    \"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return audio_array  # No augmentation applied\n",
    "    \n",
    "    # Convert to tensor for torchaudio transforms\n",
    "    audio_tensor = torch.from_numpy(audio_array).float()\n",
    "    original_length = len(audio_tensor)\n",
    "    \n",
    "    # Track which augmentations were applied for debugging\n",
    "    applied_augmentations = []\n",
    "    \n",
    "    # 1. Add background noise (30% chance if augmenting)\n",
    "    if np.random.random() < 0.3:\n",
    "        noise_level = np.random.uniform(0.001, 0.01)\n",
    "        noise = torch.randn_like(audio_tensor) * noise_level\n",
    "        audio_tensor = audio_tensor + noise\n",
    "        applied_augmentations.append(f\"noise({noise_level:.3f})\")\n",
    "    \n",
    "    # 2. Time masking (25% chance if augmenting)\n",
    "    if np.random.random() < 0.25 and original_length > 1000:\n",
    "        mask_length = np.random.randint(50, 300)\n",
    "        mask_start = np.random.randint(0, max(1, original_length - mask_length))\n",
    "        audio_tensor[mask_start:mask_start + mask_length] = 0\n",
    "        applied_augmentations.append(f\"time_mask({mask_length})\")\n",
    "    \n",
    "    # 3. Pitch shift (20% chance if augmenting)\n",
    "    if np.random.random() < 0.2:\n",
    "        try:\n",
    "            n_steps = np.random.choice([-2, -1, 1, 2])\n",
    "            pitch_shift = T.PitchShift(sample_rate, n_steps=n_steps)\n",
    "            audio_tensor = pitch_shift(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"pitch({n_steps})\")\n",
    "        except Exception as e:\n",
    "            # Fallback to simple resampling if pitch shift fails\n",
    "            try:\n",
    "                speed_factor = 1.0 + (n_steps * 0.1)\n",
    "                new_length = int(original_length / speed_factor)\n",
    "                audio_tensor = torch.nn.functional.interpolate(\n",
    "                    audio_tensor.unsqueeze(0).unsqueeze(0), \n",
    "                    size=new_length, \n",
    "                    mode='linear'\n",
    "                ).squeeze(0).squeeze(0)\n",
    "                if len(audio_tensor) > original_length:\n",
    "                    audio_tensor = audio_tensor[:original_length]\n",
    "                else:\n",
    "                    audio_tensor = torch.nn.functional.pad(\n",
    "                        audio_tensor, \n",
    "                        (0, original_length - len(audio_tensor))\n",
    "                    )\n",
    "                applied_augmentations.append(f\"pitch_approx({n_steps})\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 4. Time stretching - speed up/slow down (25% chance if augmenting)\n",
    "    if np.random.random() < 0.25 and original_length > 500:\n",
    "        rate = np.random.uniform(0.85, 1.15)  # 15% speed variation\n",
    "        new_length = int(original_length * rate)\n",
    "        \n",
    "        if new_length > 100:  # Ensure reasonable length\n",
    "            try:\n",
    "                # High-quality time stretching using interpolation\n",
    "                audio_tensor_stretched = torch.nn.functional.interpolate(\n",
    "                    audio_tensor.unsqueeze(0).unsqueeze(0), \n",
    "                    size=new_length, \n",
    "                    mode='linear',\n",
    "                    align_corners=False\n",
    "                ).squeeze(0).squeeze(0)\n",
    "                \n",
    "                # Trim or pad to original length to maintain consistency\n",
    "                if len(audio_tensor_stretched) > original_length:\n",
    "                    audio_tensor = audio_tensor_stretched[:original_length]\n",
    "                else:\n",
    "                    padding = torch.zeros(original_length - len(audio_tensor_stretched))\n",
    "                    audio_tensor = torch.cat([audio_tensor_stretched, padding])\n",
    "                \n",
    "                applied_augmentations.append(f\"time_stretch({rate:.2f})\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    # 5. Volume change (35% chance if augmenting)\n",
    "    if np.random.random() < 0.35:\n",
    "        gain = np.random.uniform(0.5, 1.5)\n",
    "        audio_tensor = audio_tensor * gain\n",
    "        # Clip to prevent distortion\n",
    "        audio_tensor = torch.clamp(audio_tensor, -1.0, 1.0)\n",
    "        applied_augmentations.append(f\"volume({gain:.2f})\")\n",
    "    \n",
    "    # 6. Low-pass filter - room simulation (15% chance if augmenting)\n",
    "    if np.random.random() < 0.15 and original_length > 1000:\n",
    "        try:\n",
    "            cutoff_freq = np.random.uniform(2000, 4000)  # Simulate telephone quality\n",
    "            lowpass = T.LowpassBiquad(sample_rate, cutoff_freq=cutoff_freq)\n",
    "            audio_tensor = lowpass(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"lowpass({cutoff_freq:.0f}Hz)\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # 7. High-pass filter (10% chance if augmenting) - remove low frequencies\n",
    "    if np.random.random() < 0.1 and original_length > 1000:\n",
    "        try:\n",
    "            cutoff_freq = np.random.uniform(100, 500)\n",
    "            highpass = T.HighpassBiquad(sample_rate, cutoff_freq=cutoff_freq)\n",
    "            audio_tensor = highpass(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"highpass({cutoff_freq:.0f}Hz)\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Debug: print augmentations for first few examples\n",
    "    if len(applied_augmentations) > 0 and np.random.random() < 0.01:  # 1% of augmented examples\n",
    "        print(f\"Applied augmentations: {', '.join(applied_augmentations)}\")\n",
    "    \n",
    "    return audio_tensor.numpy()\n",
    "\n",
    "# Audio augmentation settings (add these to your settings cell)\n",
    "USE_AUDIO_AUGMENTATION = True\n",
    "AUGMENTATION_PROB = 0.7  # 70% of training examples get augmentation\n",
    "\n",
    "print('Using audio augmentation:', USE_AUDIO_AUGMENTATION)\n",
    "print('Audio augmentation probability:', AUGMENTATION_PROB)\n",
    "print('Using specaugment:', base_model.config.apply_spec_augment)\n",
    "print('=== Using BOTH augmentation types: SpecAugment (feature-level) + Audio (signal-level) ===')\n",
    "\n",
    "# Modified prepare_features function with augmentation support\n",
    "def prepare_features(example, is_training=True):\n",
    "    \"\"\"\n",
    "    Prepare features with optional augmentation for training data\n",
    "    \"\"\"\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sample_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    # Apply augmentation ONLY to training data\n",
    "    if is_training and USE_AUDIO_AUGMENTATION:\n",
    "        audio_array = apply_audio_augmentation(audio_array, sample_rate, AUGMENTATION_PROB)\n",
    "    \n",
    "    # Extract features as before\n",
    "    example[\"input_features\"] = processor.feature_extractor(\n",
    "        audio_array, \n",
    "        sampling_rate=sample_rate\n",
    "    ).input_features[0]\n",
    "    \n",
    "    example[\"labels\"] = processor.tokenizer(example[\"transcription\"]).input_ids\n",
    "    example[\"token_length\"] = len(example[\"labels\"])\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Create wrapper functions for dataset mapping\n",
    "def prepare_train_features(example):\n",
    "    return prepare_features(example, is_training=True)\n",
    "\n",
    "def prepare_eval_features(example):\n",
    "    return prepare_features(example, is_training=False)\n",
    "\n",
    "print(\"Audio augmentation functions defined. Use prepare_train_features for training data and prepare_eval_features for test/dev data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf40332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating encoder: True\n",
      "Updating projection layer: True\n",
      "Updating decoder: False\n",
      "Overview to number of model parameters to be updated:\n",
      "* encoder params to update/total: 88154112 88154112\n",
      "* decoder parans to update/total: 39832320 153580800\n",
      "* overall # trainable parameters: 127986432\n",
      "*     overall # model parameters: 241734912\n"
     ]
    }
   ],
   "source": [
    "# which layers to tune\n",
    "\n",
    "print(\"Updating encoder:\", UPDATE_ENCODER)\n",
    "print(\"Updating projection layer:\", UPDATE_PROJ)\n",
    "print(\"Updating decoder:\", UPDATE_DECODER)\n",
    "\n",
    "\n",
    "base_model.model.encoder.requires_grad_(UPDATE_ENCODER)\n",
    "base_model.model.decoder.requires_grad_(UPDATE_DECODER)\n",
    "base_model.proj_out.requires_grad_(UPDATE_PROJ)\n",
    "\n",
    "print(\"Overview to number of model parameters to be updated:\")\n",
    "print('* encoder params to update/total:', count_trainable_parameters(base_model.model.encoder), base_model.model.encoder.num_parameters())\n",
    "print('* decoder parans to update/total:', count_trainable_parameters(base_model.model.decoder), base_model.model.decoder.num_parameters())\n",
    "\n",
    "print('* overall # trainable parameters:', count_trainable_parameters(base_model))\n",
    "print('*     overall # model parameters:', base_model.model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d40350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer args set, writing to: /jupyter_kernel/trained_models/en_nonstandard_tune_whisper_small_2\n"
     ]
    }
   ],
   "source": [
    "# Training Hyper Parameters\n",
    "# don't change settings here, but instead at very top!\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    include_num_input_tokens_seen=True,\n",
    "    ### on GPU, can either do fp16 or bf16 depending on specific GPU\n",
    "    fp16=USE_FP16, \n",
    "    bf16=USE_BF16, \n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    #\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    #\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    #\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    #\n",
    "    eval_on_start=EVAL_ON_START,\n",
    "    predict_with_generate=True,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    generation_max_length=MAX_GEN_LEN,\n",
    "    #\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    #\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    #\n",
    "    # only applies to polynomial schedule (constant ignores args)\n",
    "    lr_scheduler_kwargs={\n",
    "        \"lr_end\": LR_END, # The final LR.  Crucial for polynomial decay.\n",
    "        \"power\": LR_DECAY_POWER, # for decay\n",
    "        # we don't need to set the other arguments as they are already set in the args outside\n",
    "        #\"num_warmup_steps\": WARMUP_STEPS, # The number of steps for the warmup phase.\n",
    "        #\"num_training_steps\": MAX_STEPS, # The total number of training steps.\n",
    "        #\"lr_init\": 1e-5 # we take the LR setting\n",
    "    },\n",
    "\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=LR_WARMUP_STEPS, # what happens if we have this and the LR schedule args ?\n",
    "    #\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=NUM_CHECKPOINTS_TO_STORE,\n",
    "    load_best_model_at_end=True,\n",
    "    # group_by_length=True\n",
    "    # auto_find_batch_size=True\n",
    ")\n",
    "\n",
    "print('trainer args set, writing to:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39301594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=base_model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244a087",
   "metadata": {},
   "source": [
    "## Run the training\n",
    "\n",
    "Note: tensorboard doesn't show properly in jupyter notebooks, use the tensorboard_server.py tool to host a tensorboard instance on Modal, using below model training dir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd5fb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training dir: /jupyter_kernel/trained_models/en_nonstandard_tune_whisper_small_2\n"
     ]
    }
   ],
   "source": [
    "print('model training dir:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "740407b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 40:27, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Lattescore</th>\n",
       "      <th>Input Tokens Seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.416936</td>\n",
       "      <td>0.278586</td>\n",
       "      <td>0.166302</td>\n",
       "      <td>48.245614</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.796919</td>\n",
       "      <td>0.234155</td>\n",
       "      <td>0.132961</td>\n",
       "      <td>60.526316</td>\n",
       "      <td>144000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.958800</td>\n",
       "      <td>0.756722</td>\n",
       "      <td>0.240740</td>\n",
       "      <td>0.144147</td>\n",
       "      <td>61.988304</td>\n",
       "      <td>288000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.802600</td>\n",
       "      <td>0.724311</td>\n",
       "      <td>0.227360</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>64.619883</td>\n",
       "      <td>432000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.820200</td>\n",
       "      <td>0.702249</td>\n",
       "      <td>0.208163</td>\n",
       "      <td>0.121648</td>\n",
       "      <td>67.836257</td>\n",
       "      <td>576000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.882900</td>\n",
       "      <td>0.675111</td>\n",
       "      <td>0.213495</td>\n",
       "      <td>0.128683</td>\n",
       "      <td>66.959064</td>\n",
       "      <td>720000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.662475</td>\n",
       "      <td>0.201840</td>\n",
       "      <td>0.119898</td>\n",
       "      <td>69.298246</td>\n",
       "      <td>863520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.706600</td>\n",
       "      <td>0.664432</td>\n",
       "      <td>0.201682</td>\n",
       "      <td>0.122105</td>\n",
       "      <td>69.298246</td>\n",
       "      <td>1007520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.663069</td>\n",
       "      <td>0.202475</td>\n",
       "      <td>0.121708</td>\n",
       "      <td>67.836257</td>\n",
       "      <td>1151520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.466500</td>\n",
       "      <td>0.660234</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.119130</td>\n",
       "      <td>68.128655</td>\n",
       "      <td>1295520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>0.659746</td>\n",
       "      <td>0.197570</td>\n",
       "      <td>0.117026</td>\n",
       "      <td>69.298246</td>\n",
       "      <td>1439520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.428600</td>\n",
       "      <td>0.659702</td>\n",
       "      <td>0.197718</td>\n",
       "      <td>0.117189</td>\n",
       "      <td>69.005848</td>\n",
       "      <td>1583040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.453200</td>\n",
       "      <td>0.659704</td>\n",
       "      <td>0.197192</td>\n",
       "      <td>0.116947</td>\n",
       "      <td>69.298246</td>\n",
       "      <td>1727040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2786\n",
      "Adjusted CER: 0.1663\n",
      "LATTEScore: 48.25%\n",
      "Un-adjusted WER: 0.2915\n",
      "Un-adjusted CER: 0.1788\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2342\n",
      "Adjusted CER: 0.1330\n",
      "LATTEScore: 60.53%\n",
      "Un-adjusted WER: 0.2394\n",
      "Un-adjusted CER: 0.1330\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2407\n",
      "Adjusted CER: 0.1441\n",
      "LATTEScore: 61.99%\n",
      "Un-adjusted WER: 0.2798\n",
      "Un-adjusted CER: 0.1715\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2274\n",
      "Adjusted CER: 0.1357\n",
      "LATTEScore: 64.62%\n",
      "Un-adjusted WER: 0.2512\n",
      "Un-adjusted CER: 0.1427\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2082\n",
      "Adjusted CER: 0.1216\n",
      "LATTEScore: 67.84%\n",
      "Un-adjusted WER: 0.2476\n",
      "Un-adjusted CER: 0.1524\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2135\n",
      "Adjusted CER: 0.1287\n",
      "LATTEScore: 66.96%\n",
      "Un-adjusted WER: 0.2769\n",
      "Un-adjusted CER: 0.1776\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2018\n",
      "Adjusted CER: 0.1199\n",
      "LATTEScore: 69.30%\n",
      "Un-adjusted WER: 0.2672\n",
      "Un-adjusted CER: 0.1653\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2017\n",
      "Adjusted CER: 0.1221\n",
      "LATTEScore: 69.30%\n",
      "Un-adjusted WER: 0.2659\n",
      "Un-adjusted CER: 0.1763\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2025\n",
      "Adjusted CER: 0.1217\n",
      "LATTEScore: 67.84%\n",
      "Un-adjusted WER: 0.2551\n",
      "Un-adjusted CER: 0.1688\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.2002\n",
      "Adjusted CER: 0.1191\n",
      "LATTEScore: 68.13%\n",
      "Un-adjusted WER: 0.2528\n",
      "Un-adjusted CER: 0.1662\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.1976\n",
      "Adjusted CER: 0.1170\n",
      "LATTEScore: 69.30%\n",
      "Un-adjusted WER: 0.2502\n",
      "Un-adjusted CER: 0.1641\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.1977\n",
      "Adjusted CER: 0.1172\n",
      "LATTEScore: 69.01%\n",
      "Un-adjusted WER: 0.2503\n",
      "Un-adjusted CER: 0.1643\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.1972\n",
      "Adjusted CER: 0.1169\n",
      "LATTEScore: 69.30%\n",
      "Un-adjusted WER: 0.2498\n",
      "Un-adjusted CER: 0.1640\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.6770224614938101, metrics={'train_runtime': 2553.761, 'train_samples_per_second': 2.819, 'train_steps_per_second': 0.235, 'total_flos': 2.07666054070272e+18, 'train_loss': 0.6770224614938101, 'epoch': 2.2988505747126435, 'num_input_tokens_seen': 1727040000})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train from scratch\n",
    "trainer.train()\n",
    "\n",
    "# # alternatively, you can continue training if a previous job was interrupted\n",
    "#trainer.train(resume_from_checkpoint = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8842a",
   "metadata": {},
   "source": [
    "## Post-Training Evaluation\n",
    "\n",
    "when you run this after your training has finished it will use the best checkpoint (because we set \"load_best_model_at_end=True\" in the trainer args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2780af0",
   "metadata": {},
   "source": [
    "### On DEV set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eeecbc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.1972\n",
      "Adjusted CER: 0.1169\n",
      "LATTEScore: 69.30%\n",
      "Un-adjusted WER: 0.2498\n",
      "Un-adjusted CER: 0.1640\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6597035527229309,\n",
       " 'eval_wer': 0.19719239518466153,\n",
       " 'eval_cer': 0.1169473428980371,\n",
       " 'eval_lattescore': 69.2982456140351,\n",
       " 'eval_runtime': 116.9009,\n",
       " 'eval_samples_per_second': 2.926,\n",
       " 'eval_steps_per_second': 0.368,\n",
       " 'epoch': 2.2988505747126435,\n",
       " 'num_input_tokens_seen': 1727040000}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (should give the same result shown in trainig progress on dev set)\n",
    "trainer.evaluate(dev_dataset, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d05701",
   "metadata": {},
   "source": [
    "### On TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e55c38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.1228\n",
      "Adjusted CER: 0.0681\n",
      "LATTEScore: 77.73%\n",
      "Un-adjusted WER: 0.1358\n",
      "Un-adjusted CER: 0.0717\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5866317749023438,\n",
       " 'eval_wer': 0.12280530354136088,\n",
       " 'eval_cer': 0.06807830036387313,\n",
       " 'eval_lattescore': 77.73049645390071,\n",
       " 'eval_runtime': 242.9586,\n",
       " 'eval_samples_per_second': 2.902,\n",
       " 'eval_steps_per_second': 0.366,\n",
       " 'epoch': 2.2988505747126435,\n",
       " 'num_input_tokens_seen': 1727040000}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run on dev-set \n",
    "# (should give the same result shown in trainig progress on dev set)\n",
    "trainer.evaluate(test_dataset, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d1cc2f8-412d-4e50-8690-1da2893ea235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f549e1f9ff49dda3325c74a5c9c3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "speaker_metadata.tsv:   0%|          | 0.00/9.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c1590a7b5c4ca28ee21363d24e719b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker metadata loaded: (52, 6)\n",
      "Columns: ['speaker_id', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "  speaker_id  gender    age  \\\n",
      "0     KES001  Female  30-40   \n",
      "1     KES002  Female  30-40   \n",
      "2     KES003    Male  25-30   \n",
      "3     KES004    Male  25-30   \n",
      "4     KES005    Male  18-24   \n",
      "\n",
      "                          severity_speech_impairment  \\\n",
      "0                       Severe (frequent breakdowns)   \n",
      "1                       Severe (frequent breakdowns)   \n",
      "2  Profound (communication very difficult or impo...   \n",
      "3                       Severe (frequent breakdowns)   \n",
      "4           Moderate (requires effort to understand)   \n",
      "\n",
      "             type_nonstandard_speech               etiology  \n",
      "0                         Dysarthria         Cerebral Palsy  \n",
      "1                         Dysarthria         Cerebral Palsy  \n",
      "2  Stuttering (Disfluency Disorders)         Cerebral Palsy  \n",
      "3  Stuttering (Disfluency Disorders)  Neurological disorder  \n",
      "4  Stuttering (Disfluency Disorders)  Neurological disorder  \n",
      "Generating predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.1972\n",
      "Adjusted CER: 0.1169\n",
      "LATTEScore: 69.30%\n",
      "Un-adjusted WER: 0.2498\n",
      "Un-adjusted CER: 0.1640\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.1228\n",
      "Adjusted CER: 0.0681\n",
      "LATTEScore: 77.73%\n",
      "Un-adjusted WER: 0.1358\n",
      "Un-adjusted CER: 0.0717\n",
      "===============\n",
      "Dev WER: 0.361 | Word Accuracy: 63.9% | LATTEScore: 50.6%\n",
      "Test WER: 0.274 | Word Accuracy: 72.6% | LATTEScore: 64.4%\n",
      "\n",
      "Dev DataFrame shape: (342, 11)\n",
      "Columns: ['speaker_id', 'reference', 'prediction', 'wer', 'word_accuracy', 'lattescore_meaning_preserved', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "\n",
      "Test DataFrame shape: (705, 11)\n",
      "Columns: ['speaker_id', 'reference', 'prediction', 'wer', 'word_accuracy', 'lattescore_meaning_preserved', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "\n",
      "=== Model Deployment Analysis ===\n",
      "LATTEScore: 64.4%\n",
      "Deployment Threshold: 80.0%\n",
      "❌ RECOMMENDATION: Model does not meet quality standards\n",
      "   Consider: More training data, hyperparameter tuning, or different architecture\n",
      "\n",
      "=== FILES SAVED ===\n",
      "dev_predictions.csv: 342 samples\n",
      "test_predictions.csv: 705 samples\n",
      "\n",
      "=== DATA PREVIEW ===\n",
      "  speaker_id                                          reference  \\\n",
      "0     KES004  it seems like some some fish or some seafood i...   \n",
      "1     KES004  maybe it is kinda some milk or some some some ...   \n",
      "2     KES004  it s a cake of course but am not a good fan of...   \n",
      "3     KES004  evening walks maybe around the city or in the ...   \n",
      "4     KES004  maasai culture of course you will find them do...   \n",
      "5     KES004  i can see a giraffe there i think it it it is ...   \n",
      "6     KES004  i can say thats a a burger there on the left h...   \n",
      "7     KES004  fancy and large it is a a large fish there som...   \n",
      "8     KES004  the maasai the kids i think they were in a gat...   \n",
      "9     KES004  whenever i go to the local market i make sure ...   \n",
      "\n",
      "                                          prediction       wer  \\\n",
      "0  It seems like some some some some some some so...  7.933333   \n",
      "1  Maybe it is kind of some meal or some some som...  3.774194   \n",
      "2  It’s a cake of course but I’m not a fan of cak...  0.375000   \n",
      "3  Evening walks maybe, around the city or in the...  0.242424   \n",
      "4  Maasai culture of course you you you will find...  0.382353   \n",
      "5  I can see a giraffe there. I think it it it it...  0.428571   \n",
      "6  I can say that that's a a burger there, there ...  3.896552   \n",
      "7  Fancy and lads it would be a lads fish there s...  0.291667   \n",
      "8  The the Maasai the kings I think the the they ...  0.461538   \n",
      "9  Whenever I go to to the local market, I I make...  0.317073   \n",
      "\n",
      "   lattescore_meaning_preserved  \n",
      "0                             0  \n",
      "1                             0  \n",
      "2                             0  \n",
      "3                             1  \n",
      "4                             0  \n",
      "5                             0  \n",
      "6                             0  \n",
      "7                             1  \n",
      "8                             0  \n",
      "9                             0  \n",
      "\n",
      "=== DOWNLOAD LINKS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='dev_predictions.csv' target='_blank'>dev_predictions.csv</a><br>"
      ],
      "text/plain": [
       "/nairobo_innovation_sprint/dev_predictions.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='test_predictions.csv' target='_blank'>test_predictions.csv</a><br>"
      ],
      "text/plain": [
       "/nairobo_innovation_sprint/test_predictions.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEXT STEPS ===\n",
      "1. Analyze LATTEScore by speaker metadata (etiology, severity, gender)\n",
      "2. Compare LATTEScore with WER to see if meaning preservation differs from word accuracy\n",
      "3. Use LATTEScore for model deployment decisions\n",
      "4. Calculate LATTEScore breakdown by speaker characteristics\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import FileLink, display\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Load external speaker metadata directly ---\n",
    "metadata_url = \"https://huggingface.co/datasets/cdli/kenyan_english_nonstandard_speech_v0.9/resolve/main/speaker_metadata.tsv\"\n",
    "speaker_metadata_ds = load_dataset(\"csv\", data_files=metadata_url, sep=\"\\t\")[\"train\"]\n",
    "speaker_metadata = pd.DataFrame(speaker_metadata_ds)\n",
    "\n",
    "# Drop unwanted columns\n",
    "speaker_metadata = speaker_metadata.drop(columns=[\"comments\", \"slp_id\"], errors=\"ignore\")\n",
    "\n",
    "print(\"Speaker metadata loaded:\", speaker_metadata.shape)\n",
    "print(\"Columns:\", speaker_metadata.columns.tolist())\n",
    "print(speaker_metadata.head())\n",
    "\n",
    "# --- Function to calculate individual WER/accuracy ---\n",
    "def calculate_individual_wer(prediction, reference):\n",
    "    return wer_metric.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "# --- LATTEScore calculation function ---\n",
    "def calculate_lattescore(prediction, reference, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Calculate whether meaning is preserved for a single transcript pair\n",
    "    Returns 1 if meaning preserved, 0 if meaning lost\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Skip empty strings\n",
    "        if not reference.strip() or not prediction.strip():\n",
    "            return 0\n",
    "            \n",
    "        # Get sentence embeddings\n",
    "        emb_ref = model.encode(reference, convert_to_tensor=True)\n",
    "        emb_pred = model.encode(prediction, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(emb_ref, emb_pred).item()\n",
    "        \n",
    "        # Consider meaning preserved if similarity > threshold\n",
    "        return 1 if similarity > similarity_threshold else 0\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: use WER-based approximation\n",
    "        wer = wer_metric.compute(predictions=[prediction], references=[reference])\n",
    "        # Conservative threshold: meaning preserved if WER < 0.3 (30%)\n",
    "        return 1 if wer < 0.3 else 0\n",
    "\n",
    "# --- You need to get predictions first! Add this: ---\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Generate predictions for dev set\n",
    "preds_dev = trainer.predict(dev_dataset)\n",
    "dev_predictions = processor.tokenizer.batch_decode(preds_dev.predictions, skip_special_tokens=True)\n",
    "dev_references = [transcript_normalizer(x) for x in dev_dataset[\"transcription\"]]\n",
    "\n",
    "# Generate predictions for test set  \n",
    "preds_test = trainer.predict(test_dataset)\n",
    "test_predictions = processor.tokenizer.batch_decode(preds_test.predictions, skip_special_tokens=True)\n",
    "test_references = [transcript_normalizer(x) for x in test_dataset[\"transcription\"]]\n",
    "\n",
    "# Create prediction dictionaries\n",
    "preds_dev_dict = {\n",
    "    \"speaker_id\": dev_dataset[\"speaker_id\"],\n",
    "    \"transcription\": dev_references,\n",
    "    \"prediction\": dev_predictions\n",
    "}\n",
    "\n",
    "preds_test_dict = {\n",
    "    \"speaker_id\": test_dataset[\"speaker_id\"], \n",
    "    \"transcription\": test_references,\n",
    "    \"prediction\": test_predictions\n",
    "}\n",
    "\n",
    "# Calculate WER for entire sets\n",
    "wer_dev = wer_metric.compute(predictions=dev_predictions, references=dev_references)\n",
    "wer_test = wer_metric.compute(predictions=test_predictions, references=test_references)\n",
    "\n",
    "# Calculate metrics for individual examples\n",
    "dev_wer_individual = [calculate_individual_wer(p, r) for p, r in zip(dev_predictions, dev_references)]\n",
    "dev_acc_individual = [(1 - wer) * 100 for wer in dev_wer_individual]\n",
    "dev_lattescore_individual = [calculate_lattescore(p, r) for p, r in zip(dev_predictions, dev_references)]\n",
    "\n",
    "test_wer_individual = [calculate_individual_wer(p, r) for p, r in zip(test_predictions, test_references)]\n",
    "test_acc_individual = [(1 - wer) * 100 for wer in test_wer_individual]\n",
    "test_lattescore_individual = [calculate_lattescore(p, r) for p, r in zip(test_predictions, test_references)]\n",
    "\n",
    "# Calculate overall LATTEScore percentages\n",
    "dev_lattescore_percent = (sum(dev_lattescore_individual) / len(dev_lattescore_individual)) * 100\n",
    "test_lattescore_percent = (sum(test_lattescore_individual) / len(test_lattescore_individual)) * 100\n",
    "\n",
    "acc_dev = (1 - wer_dev) * 100\n",
    "acc_test = (1 - wer_test) * 100\n",
    "\n",
    "print(f\"Dev WER: {wer_dev:.3f} | Word Accuracy: {acc_dev:.1f}% | LATTEScore: {dev_lattescore_percent:.1f}%\")\n",
    "print(f\"Test WER: {wer_test:.3f} | Word Accuracy: {acc_test:.1f}% | LATTEScore: {test_lattescore_percent:.1f}%\")\n",
    "\n",
    "# --- Enhanced DataFrame creation with metadata merge ---\n",
    "def create_enhanced_dataframe(dataset, wer_individual, acc_individual, lattescore_individual, dataset_name):\n",
    "    base_data = {\n",
    "        \"speaker_id\": dataset[\"speaker_id\"],\n",
    "        \"reference\": dataset[\"transcription\"],\n",
    "        \"prediction\": dataset[\"prediction\"],\n",
    "        \"wer\": wer_individual,\n",
    "        \"word_accuracy\": acc_individual,\n",
    "        \"lattescore_meaning_preserved\": lattescore_individual,  # 1=preserved, 0=lost\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(base_data)\n",
    "    df = df.merge(speaker_metadata, on=\"speaker_id\", how=\"left\")\n",
    "\n",
    "    print(f\"\\n{dataset_name} DataFrame shape: {df.shape}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    return df\n",
    "\n",
    "# Build enhanced DataFrames (THIS WAS MISSING!)\n",
    "df_dev_enhanced = create_enhanced_dataframe(preds_dev_dict, dev_wer_individual, dev_acc_individual, dev_lattescore_individual, \"Dev\")\n",
    "df_test_enhanced = create_enhanced_dataframe(preds_test_dict, test_wer_individual, test_acc_individual, test_lattescore_individual, \"Test\")\n",
    "\n",
    "# --- Model deployment analysis using LATTEScore ---\n",
    "def analyze_model_deployment(lattescore, threshold=80.0):\n",
    "    \"\"\"Analyze if model meets quality standards based on LATTEScore\"\"\"\n",
    "    print(f\"\\n=== Model Deployment Analysis ===\")\n",
    "    print(f\"LATTEScore: {lattescore:.1f}%\")\n",
    "    print(f\"Deployment Threshold: {threshold}%\")\n",
    "    \n",
    "    if lattescore >= threshold:\n",
    "        print(\"✅ RECOMMENDATION: Model meets quality standards for deployment\")\n",
    "        print(\"   The ASR model preserves meaning in most transcripts\")\n",
    "    else:\n",
    "        print(\"❌ RECOMMENDATION: Model does not meet quality standards\")\n",
    "        print(\"   Consider: More training data, hyperparameter tuning, or different architecture\")\n",
    "    \n",
    "    return lattescore >= threshold\n",
    "\n",
    "# Run deployment analysis\n",
    "deployment_ready = analyze_model_deployment(test_lattescore_percent)\n",
    "\n",
    "# --- Save only the main analysis files ---\n",
    "df_dev_enhanced.to_csv(\"dev_predictions.csv\", index=False)\n",
    "df_test_enhanced.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== FILES SAVED ===\")\n",
    "print(f\"dev_predictions.csv: {len(df_dev_enhanced)} samples\")\n",
    "print(f\"test_predictions.csv: {len(df_test_enhanced)} samples\")\n",
    "\n",
    "# --- Preview data ---\n",
    "print(\"\\n=== DATA PREVIEW ===\")\n",
    "print(df_dev_enhanced[['speaker_id', 'reference', 'prediction', 'wer', 'lattescore_meaning_preserved']].head(10))\n",
    "\n",
    "# --- Download links ---\n",
    "print(\"\\n=== DOWNLOAD LINKS ===\")\n",
    "display(FileLink(\"dev_predictions.csv\"))\n",
    "display(FileLink(\"test_predictions.csv\"))\n",
    "\n",
    "# --- Updated next steps ---\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Analyze LATTEScore by speaker metadata (etiology, severity, gender)\")\n",
    "print(\"2. Compare LATTEScore with WER to see if meaning preservation differs from word accuracy\")\n",
    "print(\"3. Use LATTEScore for model deployment decisions\")\n",
    "print(\"4. Calculate LATTEScore breakdown by speaker characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9082ef",
   "metadata": {},
   "source": [
    "## Store Model\n",
    "\n",
    "--> save best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d4454",
   "metadata": {},
   "source": [
    "### Save to your volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ff87bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: /jupyter_kernel/trained_models/en_nonstandard_tune_whisper_small_2/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with \"load_best_model_at_end=True\" set in the settings (this is the default, so don't change that), after training is completed the best model is loaded and then saved\n",
    "best_model_dir = os.path.join(OUTPUT_DIR, 'best_model')\n",
    "print(f\"Saving to: {best_model_dir}\")\n",
    "trainer.model.save_pretrained(best_model_dir, safe_serialization=True)\n",
    "trainer.tokenizer.save_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81bd7a-f6ed-43f6-8a54-2d9bd5c55abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
