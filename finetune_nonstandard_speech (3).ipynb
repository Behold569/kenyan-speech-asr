{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21c1499",
   "metadata": {},
   "source": [
    "# Fine-tune Whisper on CDLI Non-Standard Speech Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43222608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 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\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "HF_TOKEN = input()\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f87c76",
   "metadata": {},
   "source": [
    "## Settings \n",
    "\n",
    "--> adapt for your scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3ce04",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95aac5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will write model to: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_large_4\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# storage in Volume that will persist\n",
    "LOCAL_STORAGE_DIR = '/jupyter_kernel'\n",
    "\n",
    "BASE_DIR = os.path.join(LOCAL_STORAGE_DIR, 'trained_models')\n",
    "!mkdir -p {BASE_DIR}\n",
    "\n",
    "# directory for model training\n",
    "#OUTPUT_DIR = os.path.join(BASE_DIR, 'en_nonstandard_tune_whisper_small_1')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'sw_nonstandard_tune_whisper_large_4')\n",
    "\n",
    "print(f\"Will write model to: {OUTPUT_DIR}\")\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    raise ValueError(f\"Output directory already exists - if you continue this will overwrite data and may lead to strange results...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341cb64",
   "metadata": {},
   "source": [
    "### Model and Dataset settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0091bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#WHISPER_MODEL_TYPE = \"openai/whisper-tiny\" \n",
    "# WHISPER_MODEL_TYPE = \"openai/whisper-small\" \n",
    "WHISPER_MODEL_TYPE = \"openai/whisper-large-v3\" \n",
    "\n",
    "#LANGUAGE = 'en'\n",
    "#DATASET_NAME = \"cdli/kenyan_english_nonstandard_speech_v0.9\"\n",
    "\n",
    "LANGUAGE = 'sw'\n",
    "DATASET_NAME = \"cdli/kenyan_swahili_nonstandard_speech_v0.9\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c808490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which parts of the model to update\n",
    "UPDATE_ENCODER = True\n",
    "UPDATE_PROJ = True\n",
    "UPDATE_DECODER = False\n",
    "\n",
    "# Turn on SpecAugment\n",
    "USE_SPECAUGMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf2f146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model will be loaded from: openai/whisper-large-v3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######################\n",
    "## don't change these!\n",
    "######################\n",
    "\n",
    "\n",
    "TASK = \"transcribe\"\n",
    "\n",
    "BASE_MODEL_NAME = WHISPER_MODEL_TYPE\n",
    "print('Base model will be loaded from:', BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2735de",
   "metadata": {},
   "source": [
    "### Trainer Settings\n",
    "\n",
    "--> adjust as needed or keep defaults (these settings should be a good starting point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "104baa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING_STEPS = 5           # More frequent logging for large model\n",
    "SAVE_STEPS = 50             # More frequent saves due to longer training time\n",
    "\n",
    "# training duration\n",
    "MAX_EPOCHS = 3              # Fewer epochs - large models converge faster\n",
    "MAX_STEPS = 200             # Fewer steps - each step takes much longer\n",
    "\n",
    "# Learning Rate and LR Scheduler\n",
    "LEARNING_RATE = 1e-5        # Lower LR for large model stability\n",
    "LR_SCHEDULER_TYPE = 'polynomial'\n",
    "LR_WARMUP_STEPS = 20\n",
    "LR_END = 1e-8\n",
    "LR_DECAY_POWER = 4\n",
    "\n",
    "# Batch sizes\n",
    "BATCH_SIZE = 2              # Much smaller due to memory constraints\n",
    "EVAL_BATCH_SIZE = 1         # Very small for evaluation\n",
    "\n",
    "# Evaluation\n",
    "MAX_GEN_LEN = 128\n",
    "EVAL_ON_START = True\n",
    "EVAL_STEPS = 25             # More frequent evaluation\n",
    "\n",
    "# Precision\n",
    "USE_FP16 = True\n",
    "USE_BF16 = False\n",
    "\n",
    "# Checkpoints\n",
    "NUM_CHECKPOINTS_TO_STORE = 1  # Checkpoints are huge (~18 GB!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57078735",
   "metadata": {},
   "source": [
    "## Imports and Prep\n",
    "\n",
    "--> no need to change anything here, just run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "effd0080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: False\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# more efficient dataset handling\n",
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()\n",
    "\n",
    "# check if we have gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7034f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import random\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "\n",
    "import tarfile\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import evaluate\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "transcript_normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "903eac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "290a30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wer(references, predictions, normalize=True, verbose=True):\n",
    "  rs = references\n",
    "  ps = predictions\n",
    "  if normalize:\n",
    "    ps = [transcript_normalizer(x) for x in predictions]\n",
    "    rs = [transcript_normalizer(x) for x in references]\n",
    "  if verbose:\n",
    "    for r, p in zip(rs, ps):\n",
    "      print(r)\n",
    "      print(p)\n",
    "      print()\n",
    "\n",
    "  return wer_metric.compute(references=rs, predictions=ps)\n",
    "\n",
    "def compute_lattescore(references, predictions, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    LATTEScore calculation - percentage of transcripts that preserve meaning\n",
    "    Based on the paper: Large Language Models As A Proxy For Human Evaluation\n",
    "    This is a simplified version using semantic similarity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to use sentence transformers for better semantic similarity\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        preserved_count = 0\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            # Skip empty strings\n",
    "            if not ref.strip() or not pred.strip():\n",
    "                continue\n",
    "                \n",
    "            # Get sentence embeddings\n",
    "            emb_ref = model.encode(ref, convert_to_tensor=True)\n",
    "            emb_pred = model.encode(pred, convert_to_tensor=True)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = util.pytorch_cos_sim(emb_ref, emb_pred).item()\n",
    "            \n",
    "            # Consider meaning preserved if similarity > threshold\n",
    "            if similarity > similarity_threshold:\n",
    "                preserved_count += 1\n",
    "        \n",
    "        total_count = len([r for r in references if r.strip()])\n",
    "        lattescore = (preserved_count / total_count) * 100 if total_count > 0 else 0\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: use WER-based approximation if sentence-transformers not available\n",
    "        print(\"Sentence transformers not available, using WER-based LATTEScore approximation\")\n",
    "        preserved_count = 0\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            # Skip empty strings\n",
    "            if not ref.strip() or not pred.strip():\n",
    "                continue\n",
    "                \n",
    "            wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "            # Conservative threshold: meaning preserved if WER < 0.3 (30%)\n",
    "            if wer < 0.3:\n",
    "                preserved_count += 1\n",
    "        \n",
    "        total_count = len([r for r in references if r.strip()])\n",
    "        lattescore = (preserved_count / total_count) * 100 if total_count > 0 else 0\n",
    "    \n",
    "    return lattescore\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for ASR evaluation including WER, CER, and LATTEScore\n",
    "    \"\"\"\n",
    "    # for training metrics\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_strs = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_strs = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # calculate a per-example average for WER and CER\n",
    "    wers = []\n",
    "    cers = []\n",
    "    for pred_str, label_str in zip(pred_strs, label_strs):\n",
    "        p = transcript_normalizer(pred_str)\n",
    "        l = transcript_normalizer(label_str)\n",
    "        # Skip empty strings for metric calculation\n",
    "        if l.strip() and p.strip():\n",
    "            wer = wer_metric.compute(predictions=[p], references=[l])\n",
    "            cer = cer_metric.compute(predictions=[p], references=[l])\n",
    "            wers.append(wer)\n",
    "            cers.append(cer)\n",
    "\n",
    "    wer = np.mean([min(1.0, x) for x in wers]) if wers else 1.0\n",
    "    cer = np.mean([min(1.0, x) for x in cers]) if cers else 1.0\n",
    "    \n",
    "    # Calculate LATTEScore\n",
    "    lattescore = compute_lattescore(label_strs, pred_strs)\n",
    "    \n",
    "    print('=== Metrics ===')\n",
    "    print(f'Adjusted WER: {wer:.4f}')\n",
    "    print(f'Adjusted CER: {cer:.4f}')\n",
    "    print(f'LATTEScore: {lattescore:.2f}%')\n",
    "    print(f'Un-adjusted WER: {np.mean(wers) if wers else 1.0:.4f}')\n",
    "    print(f'Un-adjusted CER: {np.mean(cers) if cers else 1.0:.4f}')\n",
    "    print('===============')\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer, \n",
    "        \"cer\": cer, \n",
    "        \"lattescore\": lattescore\n",
    "    }\n",
    "\n",
    "# Optional: Add a function to analyze model quality based on LATTEScore\n",
    "def analyze_model_deployment(lattescore, threshold=80.0):\n",
    "    \"\"\"\n",
    "    Analyze if model meets quality standards for deployment based on LATTEScore\n",
    "    Using the 80% threshold from the research paper\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Model Deployment Analysis ===\")\n",
    "    print(f\"LATTEScore: {lattescore:.2f}%\")\n",
    "    print(f\"Deployment Threshold: {threshold}%\")\n",
    "    \n",
    "    if lattescore >= threshold:\n",
    "        print(\"✅ RECOMMENDATION: Model meets quality standards for deployment\")\n",
    "        print(\"   The ASR model preserves meaning in most transcripts\")\n",
    "    else:\n",
    "        print(\"❌ RECOMMENDATION: Model does not meet quality standards\")\n",
    "        print(\"   Consider: More training data, hyperparameter tuning, or different architecture\")\n",
    "    \n",
    "    return lattescore >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "105389f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name, split='test', limit_to_30_seconds=True):\n",
    "    \"\"\"\n",
    "    Load a dataset from Hugging Face Hub.\n",
    "    If limit_to_30_seconds is True, will only load examples with audio length <= 30 seconds.\n",
    "    \"\"\"\n",
    "    if split not in ['train', 'test', 'validation']:\n",
    "        raise ValueError(\"split must be one of 'train', 'test', or 'validation'\")\n",
    "    ds = datasets.load_dataset(dataset_name, split=split, streaming=False)\n",
    "    orig_len = len(ds)\n",
    "    if limit_to_30_seconds:\n",
    "        ds = ds.filter(lambda example: example['audio_length'] <= 30)\n",
    "        print(f\"Filtered dataset from {orig_len} to {len(ds)} examples with audio length <= 30 seconds\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac36c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following warning can be ignored:\n",
    "# \"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
    "# See: https://discuss.huggingface.co/t/finetuning-whisper-attention-mask-not-set-and-canot-be-inferred/97456\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb8215",
   "metadata": {},
   "source": [
    "## Download datasets and prepare features\n",
    "\n",
    "--> no need to change anything here, just run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcf0e7",
   "metadata": {},
   "source": [
    "### Optimizing some settings for dataset access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c69a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: False\n",
      "device is:  cuda\n",
      "# processors: 24\n"
     ]
    }
   ],
   "source": [
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is: ', device)\n",
    "\n",
    "# IMPORTANT! need to set to 1 to avoid the mapping to hang!\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()\n",
    "\n",
    "num_proc = min(32, os.cpu_count())\n",
    "print('# processors:', num_proc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a27923",
   "metadata": {},
   "source": [
    "### Load feature extractor\n",
    "\n",
    "--> for the model type you specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85919d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Language:  sw\n",
      "Using model: openai/whisper-large-v3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load processor\n",
    "print('Using Language: ', LANGUAGE)\n",
    "print('Using model:', WHISPER_MODEL_TYPE)\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_TYPE, language=LANGUAGE, task=TASK)\n",
    "\n",
    "# since this tokenizer isn't a FastTokenizer, so there is no point in running it with is_batched=True\n",
    "# see: processor.tokenizer.is_fast\n",
    "def prepare_features(example):\n",
    "    example[\"input_features\"] = processor.feature_extractor(example[\"audio\"][\"array\"], sampling_rate=example[\"audio\"][\"sampling_rate\"]).input_features[0]\n",
    "    example[\"labels\"] = processor.tokenizer(example[\"transcription\"]).input_ids\n",
    "    # also count number of tokens\n",
    "    example[\"token_length\"] = len(example[\"labels\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d19cd",
   "metadata": {},
   "source": [
    "### Load non-standard speech dataset\n",
    "\n",
    "We need to filter to 30 seconds, as Whisper can only train on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a020684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bea544a57b5491aaf03cf22f46f376d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a031c9143df74febaa252c0a15409a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00002.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb48edef02b646bd94f2b074ba3f6571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00001-of-00002.parquet:   0%|          | 0.00/313M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5899b70d704958b9d2e8ff095a05df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/338M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97654a2d13a44ed1aef1cb9fa461fcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00006.parquet:   0%|          | 0.00/372M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52962c8915a4d64826b8af2553fcb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00006.parquet:   0%|          | 0.00/506M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f63feb63c541a0aea97ee78c623d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00006.parquet:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f5dd1649aa40fb9b20fd69aa88c497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00006.parquet:   0%|          | 0.00/509M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2322cde4c7d54f2cbfff0f6e457f88bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00006.parquet:   0%|          | 0.00/567M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05e7137a92c4831bc07dc25e4f2889f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00006.parquet:   0%|          | 0.00/509M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530b9b2041c348a98b9c94da05d283d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2206ef79f4634b518a97513d79e1da95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820b3b4e6d674397b6b98c6fb975549a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c04d21ca74a44f2b0daf309fd483f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 3949 to 2856 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa44749d5c2941b482c9b462064b4aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/2856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TRAIN dataset with 2856 examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(DATASET_NAME, split='train', limit_to_30_seconds=True)\n",
    "train_dataset = train_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded TRAIN dataset with {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3a95a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b5101024e44fd6890d9f0265346f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 865 to 554 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199f209fdf08494593e96623d016b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TEST dataset with 554 examples\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(DATASET_NAME, split='test', limit_to_30_seconds=True)\n",
    "test_dataset = test_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded TEST dataset with {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c34fa18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afc688175364898b1806fefb33ef2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 417 to 272 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b165970e8f554f978519078e4fb03dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DEV dataset with 272 examples\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = load_dataset(DATASET_NAME, split='validation', limit_to_30_seconds=True)\n",
    "dev_dataset = dev_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded DEV dataset with {len(dev_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d1182",
   "metadata": {},
   "source": [
    "## Prepare Trainer\n",
    "\n",
    "--> no need to change anything here, just run\n",
    "\n",
    "Whenever something is changed in the settings, you need to rerun this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b402d733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Language:  sw\n",
      "Using model: openai/whisper-large-v3\n",
      "language set to: sw\n"
     ]
    }
   ],
   "source": [
    "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
    "_ = base_model.to(device)\n",
    "print('Using Language: ', LANGUAGE)\n",
    "print('Using model:', WHISPER_MODEL_TYPE)\n",
    "\n",
    "# ensure task and language for training\n",
    "base_model.generation_config.language = LANGUAGE\n",
    "base_model.generation_config.task = TASK\n",
    "base_model.generation_config.forced_decoder_ids = None\n",
    "base_model.config.forced_decoder_ids = None\n",
    "# to use gradient checkpointing\n",
    "base_model.config.use_cache = False\n",
    "print('language set to:', base_model.generation_config.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f46b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using specaugment: True\n",
      "Using audio augmentation: True\n",
      "Audio augmentation probability: 0.7\n",
      "Using specaugment: True\n",
      "=== Using BOTH augmentation types: SpecAugment (feature-level) + Audio (signal-level) ===\n",
      "Audio augmentation functions defined. Use prepare_train_features for training data and prepare_eval_features for test/dev data.\n"
     ]
    }
   ],
   "source": [
    "# Add SpecAugment\n",
    "if USE_SPECAUGMENT:\n",
    "    base_model.config.apply_spec_augment = USE_SPECAUGMENT\n",
    "\n",
    "    # Specaugment (use default settings, as per paper)\n",
    "    # time masking\n",
    "    base_model.config.mask_time_prob = 0.05\n",
    "    base_model.config.mask_time_length = 10\n",
    "    base_model.config.mask_time_min_masks = 2\n",
    "\n",
    "    # feature masking\n",
    "    base_model.config.mask_feature_prob = 0.05 # def: 0\n",
    "    base_model.config.mask_feature_length = 10\n",
    "    base_model.config.mask_feature_min_masks = 2 # def: 0\n",
    "\n",
    "print('Using specaugment:', base_model.config.apply_spec_augment)\n",
    "\n",
    "# Add Audio Augmentation functions\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "def apply_audio_augmentation(audio_array, sample_rate=16000, augmentation_prob=0.7):\n",
    "    \"\"\"\n",
    "    Apply audio augmentations to training data\n",
    "    augmentation_prob: probability of applying any augmentation to an example\n",
    "    \"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return audio_array  # No augmentation applied\n",
    "    \n",
    "    # Convert to tensor for torchaudio transforms\n",
    "    audio_tensor = torch.from_numpy(audio_array).float()\n",
    "    original_length = len(audio_tensor)\n",
    "    \n",
    "    # Track which augmentations were applied for debugging\n",
    "    applied_augmentations = []\n",
    "    \n",
    "    # 1. Add background noise (30% chance if augmenting)\n",
    "    if np.random.random() < 0.3:\n",
    "        noise_level = np.random.uniform(0.001, 0.01)\n",
    "        noise = torch.randn_like(audio_tensor) * noise_level\n",
    "        audio_tensor = audio_tensor + noise\n",
    "        applied_augmentations.append(f\"noise({noise_level:.3f})\")\n",
    "    \n",
    "    # 2. Time masking (25% chance if augmenting)\n",
    "    if np.random.random() < 0.25 and original_length > 1000:\n",
    "        mask_length = np.random.randint(50, 300)\n",
    "        mask_start = np.random.randint(0, max(1, original_length - mask_length))\n",
    "        audio_tensor[mask_start:mask_start + mask_length] = 0\n",
    "        applied_augmentations.append(f\"time_mask({mask_length})\")\n",
    "    \n",
    "    # 3. Pitch shift (20% chance if augmenting)\n",
    "    if np.random.random() < 0.2:\n",
    "        try:\n",
    "            n_steps = np.random.choice([-2, -1, 1, 2])\n",
    "            pitch_shift = T.PitchShift(sample_rate, n_steps=n_steps)\n",
    "            audio_tensor = pitch_shift(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"pitch({n_steps})\")\n",
    "        except Exception as e:\n",
    "            # Fallback to simple resampling if pitch shift fails\n",
    "            try:\n",
    "                speed_factor = 1.0 + (n_steps * 0.1)\n",
    "                new_length = int(original_length / speed_factor)\n",
    "                audio_tensor = torch.nn.functional.interpolate(\n",
    "                    audio_tensor.unsqueeze(0).unsqueeze(0), \n",
    "                    size=new_length, \n",
    "                    mode='linear'\n",
    "                ).squeeze(0).squeeze(0)\n",
    "                if len(audio_tensor) > original_length:\n",
    "                    audio_tensor = audio_tensor[:original_length]\n",
    "                else:\n",
    "                    audio_tensor = torch.nn.functional.pad(\n",
    "                        audio_tensor, \n",
    "                        (0, original_length - len(audio_tensor))\n",
    "                    )\n",
    "                applied_augmentations.append(f\"pitch_approx({n_steps})\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 4. Time stretching - speed up/slow down (25% chance if augmenting)\n",
    "    if np.random.random() < 0.25 and original_length > 500:\n",
    "        rate = np.random.uniform(0.85, 1.15)  # 15% speed variation\n",
    "        new_length = int(original_length * rate)\n",
    "        \n",
    "        if new_length > 100:  # Ensure reasonable length\n",
    "            try:\n",
    "                # High-quality time stretching using interpolation\n",
    "                audio_tensor_stretched = torch.nn.functional.interpolate(\n",
    "                    audio_tensor.unsqueeze(0).unsqueeze(0), \n",
    "                    size=new_length, \n",
    "                    mode='linear',\n",
    "                    align_corners=False\n",
    "                ).squeeze(0).squeeze(0)\n",
    "                \n",
    "                # Trim or pad to original length to maintain consistency\n",
    "                if len(audio_tensor_stretched) > original_length:\n",
    "                    audio_tensor = audio_tensor_stretched[:original_length]\n",
    "                else:\n",
    "                    padding = torch.zeros(original_length - len(audio_tensor_stretched))\n",
    "                    audio_tensor = torch.cat([audio_tensor_stretched, padding])\n",
    "                \n",
    "                applied_augmentations.append(f\"time_stretch({rate:.2f})\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    # 5. Volume change (35% chance if augmenting)\n",
    "    if np.random.random() < 0.35:\n",
    "        gain = np.random.uniform(0.5, 1.5)\n",
    "        audio_tensor = audio_tensor * gain\n",
    "        # Clip to prevent distortion\n",
    "        audio_tensor = torch.clamp(audio_tensor, -1.0, 1.0)\n",
    "        applied_augmentations.append(f\"volume({gain:.2f})\")\n",
    "    \n",
    "    # 6. Low-pass filter - room simulation (15% chance if augmenting)\n",
    "    if np.random.random() < 0.15 and original_length > 1000:\n",
    "        try:\n",
    "            cutoff_freq = np.random.uniform(2000, 4000)  # Simulate telephone quality\n",
    "            lowpass = T.LowpassBiquad(sample_rate, cutoff_freq=cutoff_freq)\n",
    "            audio_tensor = lowpass(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"lowpass({cutoff_freq:.0f}Hz)\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # 7. High-pass filter (10% chance if augmenting) - remove low frequencies\n",
    "    if np.random.random() < 0.1 and original_length > 1000:\n",
    "        try:\n",
    "            cutoff_freq = np.random.uniform(100, 500)\n",
    "            highpass = T.HighpassBiquad(sample_rate, cutoff_freq=cutoff_freq)\n",
    "            audio_tensor = highpass(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"highpass({cutoff_freq:.0f}Hz)\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Debug: print augmentations for first few examples\n",
    "    if len(applied_augmentations) > 0 and np.random.random() < 0.01:  # 1% of augmented examples\n",
    "        print(f\"Applied augmentations: {', '.join(applied_augmentations)}\")\n",
    "    \n",
    "    return audio_tensor.numpy()\n",
    "\n",
    "# Audio augmentation settings (add these to your settings cell)\n",
    "USE_AUDIO_AUGMENTATION = True\n",
    "AUGMENTATION_PROB = 0.7  # 70% of training examples get augmentation\n",
    "\n",
    "print('Using audio augmentation:', USE_AUDIO_AUGMENTATION)\n",
    "print('Audio augmentation probability:', AUGMENTATION_PROB)\n",
    "print('Using specaugment:', base_model.config.apply_spec_augment)\n",
    "print('=== Using BOTH augmentation types: SpecAugment (feature-level) + Audio (signal-level) ===')\n",
    "\n",
    "# Modified prepare_features function with augmentation support\n",
    "def prepare_features(example, is_training=True):\n",
    "    \"\"\"\n",
    "    Prepare features with optional augmentation for training data\n",
    "    \"\"\"\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sample_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    # Apply augmentation ONLY to training data\n",
    "    if is_training and USE_AUDIO_AUGMENTATION:\n",
    "        audio_array = apply_audio_augmentation(audio_array, sample_rate, AUGMENTATION_PROB)\n",
    "    \n",
    "    # Extract features as before\n",
    "    example[\"input_features\"] = processor.feature_extractor(\n",
    "        audio_array, \n",
    "        sampling_rate=sample_rate\n",
    "    ).input_features[0]\n",
    "    \n",
    "    example[\"labels\"] = processor.tokenizer(example[\"transcription\"]).input_ids\n",
    "    example[\"token_length\"] = len(example[\"labels\"])\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Create wrapper functions for dataset mapping\n",
    "def prepare_train_features(example):\n",
    "    return prepare_features(example, is_training=True)\n",
    "\n",
    "def prepare_eval_features(example):\n",
    "    return prepare_features(example, is_training=False)\n",
    "\n",
    "print(\"Audio augmentation functions defined. Use prepare_train_features for training data and prepare_eval_features for test/dev data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf40332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating encoder: True\n",
      "Updating projection layer: True\n",
      "Updating decoder: False\n",
      "Overview to number of model parameters to be updated:\n",
      "* encoder params to update/total: 636968960 636968960\n",
      "* decoder parans to update/total: 66388480 906521600\n",
      "* overall # trainable parameters: 703357440\n",
      "*     overall # model parameters: 1543490560\n"
     ]
    }
   ],
   "source": [
    "# which layers to tune\n",
    "\n",
    "print(\"Updating encoder:\", UPDATE_ENCODER)\n",
    "print(\"Updating projection layer:\", UPDATE_PROJ)\n",
    "print(\"Updating decoder:\", UPDATE_DECODER)\n",
    "\n",
    "\n",
    "base_model.model.encoder.requires_grad_(UPDATE_ENCODER)\n",
    "base_model.model.decoder.requires_grad_(UPDATE_DECODER)\n",
    "base_model.proj_out.requires_grad_(UPDATE_PROJ)\n",
    "\n",
    "print(\"Overview to number of model parameters to be updated:\")\n",
    "print('* encoder params to update/total:', count_trainable_parameters(base_model.model.encoder), base_model.model.encoder.num_parameters())\n",
    "print('* decoder parans to update/total:', count_trainable_parameters(base_model.model.decoder), base_model.model.decoder.num_parameters())\n",
    "\n",
    "print('* overall # trainable parameters:', count_trainable_parameters(base_model))\n",
    "print('*     overall # model parameters:', base_model.model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d40350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer args set, writing to: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_large_4\n"
     ]
    }
   ],
   "source": [
    "# Training Hyper Parameters\n",
    "# don't change settings here, but instead at very top!\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    include_num_input_tokens_seen=True,\n",
    "    ### on GPU, can either do fp16 or bf16 depending on specific GPU\n",
    "    fp16=USE_FP16, \n",
    "    bf16=USE_BF16, \n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    #\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    #\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    #\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    #\n",
    "    eval_on_start=EVAL_ON_START,\n",
    "    predict_with_generate=True,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    generation_max_length=MAX_GEN_LEN,\n",
    "    #\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    #\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    #\n",
    "    # only applies to polynomial schedule (constant ignores args)\n",
    "    lr_scheduler_kwargs={\n",
    "        \"lr_end\": LR_END, # The final LR.  Crucial for polynomial decay.\n",
    "        \"power\": LR_DECAY_POWER, # for decay\n",
    "        # we don't need to set the other arguments as they are already set in the args outside\n",
    "        #\"num_warmup_steps\": WARMUP_STEPS, # The number of steps for the warmup phase.\n",
    "        #\"num_training_steps\": MAX_STEPS, # The total number of training steps.\n",
    "        #\"lr_init\": 1e-5 # we take the LR setting\n",
    "    },\n",
    "\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=LR_WARMUP_STEPS, # what happens if we have this and the LR schedule args ?\n",
    "    #\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=NUM_CHECKPOINTS_TO_STORE,\n",
    "    load_best_model_at_end=True,\n",
    "    # group_by_length=True\n",
    "    # auto_find_batch_size=True\n",
    ")\n",
    "\n",
    "print('trainer args set, writing to:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39301594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=base_model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244a087",
   "metadata": {},
   "source": [
    "## Run the training\n",
    "\n",
    "Note: tensorboard doesn't show properly in jupyter notebooks, use the tensorboard_server.py tool to host a tensorboard instance on Modal, using below model training dir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd5fb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training dir: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_large_4\n"
     ]
    }
   ],
   "source": [
    "print('model training dir:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "740407b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 1:52:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Lattescore</th>\n",
       "      <th>Input Tokens Seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.849143</td>\n",
       "      <td>0.722794</td>\n",
       "      <td>0.304201</td>\n",
       "      <td>1.838235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.398700</td>\n",
       "      <td>1.506467</td>\n",
       "      <td>0.630979</td>\n",
       "      <td>0.221502</td>\n",
       "      <td>4.044118</td>\n",
       "      <td>19200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.587400</td>\n",
       "      <td>1.418386</td>\n",
       "      <td>0.621938</td>\n",
       "      <td>0.229396</td>\n",
       "      <td>5.147059</td>\n",
       "      <td>38400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.783200</td>\n",
       "      <td>1.336483</td>\n",
       "      <td>0.602169</td>\n",
       "      <td>0.221205</td>\n",
       "      <td>4.779412</td>\n",
       "      <td>57600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.034600</td>\n",
       "      <td>1.303896</td>\n",
       "      <td>0.585901</td>\n",
       "      <td>0.214827</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>76800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.350300</td>\n",
       "      <td>1.281153</td>\n",
       "      <td>0.582280</td>\n",
       "      <td>0.221534</td>\n",
       "      <td>7.352941</td>\n",
       "      <td>96000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.097800</td>\n",
       "      <td>1.272910</td>\n",
       "      <td>0.583561</td>\n",
       "      <td>0.221364</td>\n",
       "      <td>7.352941</td>\n",
       "      <td>115200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.264900</td>\n",
       "      <td>1.270432</td>\n",
       "      <td>0.581967</td>\n",
       "      <td>0.221552</td>\n",
       "      <td>7.720588</td>\n",
       "      <td>134400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.182300</td>\n",
       "      <td>1.269624</td>\n",
       "      <td>0.582217</td>\n",
       "      <td>0.221454</td>\n",
       "      <td>7.352941</td>\n",
       "      <td>153600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.7228\n",
      "Adjusted CER: 0.3042\n",
      "LATTEScore: 1.84%\n",
      "Un-adjusted WER: 0.7964\n",
      "Un-adjusted CER: 0.3585\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.6310\n",
      "Adjusted CER: 0.2215\n",
      "LATTEScore: 4.04%\n",
      "Un-adjusted WER: 0.6455\n",
      "Un-adjusted CER: 0.2227\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.6219\n",
      "Adjusted CER: 0.2294\n",
      "LATTEScore: 5.15%\n",
      "Un-adjusted WER: 0.7031\n",
      "Un-adjusted CER: 0.2655\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.6022\n",
      "Adjusted CER: 0.2212\n",
      "LATTEScore: 4.78%\n",
      "Un-adjusted WER: 0.6323\n",
      "Un-adjusted CER: 0.2412\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5859\n",
      "Adjusted CER: 0.2148\n",
      "LATTEScore: 6.25%\n",
      "Un-adjusted WER: 0.6644\n",
      "Un-adjusted CER: 0.2618\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5823\n",
      "Adjusted CER: 0.2215\n",
      "LATTEScore: 7.35%\n",
      "Un-adjusted WER: 0.7025\n",
      "Un-adjusted CER: 0.2856\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5836\n",
      "Adjusted CER: 0.2214\n",
      "LATTEScore: 7.35%\n",
      "Un-adjusted WER: 0.7041\n",
      "Un-adjusted CER: 0.2854\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5820\n",
      "Adjusted CER: 0.2216\n",
      "LATTEScore: 7.72%\n",
      "Un-adjusted WER: 0.7031\n",
      "Un-adjusted CER: 0.2856\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5822\n",
      "Adjusted CER: 0.2215\n",
      "LATTEScore: 7.35%\n",
      "Un-adjusted WER: 0.7033\n",
      "Un-adjusted CER: 0.2855\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.4087182784080505, metrics={'train_runtime': 7612.5076, 'train_samples_per_second': 0.053, 'train_steps_per_second': 0.026, 'total_flos': 1.358999322624e+18, 'train_loss': 1.4087182784080505, 'epoch': 0.1400560224089636, 'num_input_tokens_seen': 153600000})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train from scratch\n",
    "trainer.train()\n",
    "\n",
    "# # alternatively, you can continue training if a previous job was interrupted\n",
    "# trainer.train(resume_from_checkpoint = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8842a",
   "metadata": {},
   "source": [
    "## Post-Training Evaluation\n",
    "\n",
    "when you run this after your training has finished it will use the best checkpoint (because we set \"load_best_model_at_end=True\" in the trainer args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2780af0",
   "metadata": {},
   "source": [
    "### On DEV set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eeecbc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5859\n",
      "Adjusted CER: 0.2148\n",
      "LATTEScore: 6.25%\n",
      "Un-adjusted WER: 0.6644\n",
      "Un-adjusted CER: 0.2618\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3038960695266724,\n",
       " 'eval_wer': 0.5859010886650711,\n",
       " 'eval_cer': 0.21482694968073462,\n",
       " 'eval_lattescore': 6.25,\n",
       " 'eval_runtime': 804.9829,\n",
       " 'eval_samples_per_second': 0.338,\n",
       " 'eval_steps_per_second': 0.338,\n",
       " 'epoch': 0.1400560224089636,\n",
       " 'num_input_tokens_seen': 153600000}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (should give the same result shown in trainig progress on dev set)\n",
    "trainer.evaluate(dev_dataset, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d05701",
   "metadata": {},
   "source": [
    "### On TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e55c38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4158\n",
      "Adjusted CER: 0.1528\n",
      "LATTEScore: 22.02%\n",
      "Un-adjusted WER: 0.4594\n",
      "Un-adjusted CER: 0.1701\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0432450771331787,\n",
       " 'eval_wer': 0.41582398430788187,\n",
       " 'eval_cer': 0.1527811293851456,\n",
       " 'eval_lattescore': 22.021660649819495,\n",
       " 'eval_runtime': 1831.4445,\n",
       " 'eval_samples_per_second': 0.302,\n",
       " 'eval_steps_per_second': 0.302,\n",
       " 'epoch': 0.1400560224089636,\n",
       " 'num_input_tokens_seen': 153600000}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run on dev-set \n",
    "# (should give the same result shown in trainig progress on dev set)\n",
    "trainer.evaluate(test_dataset, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8333580-7ae9-47dc-9d03-46556f2c2e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2c4625607242b5b1fdbddd867c721c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "speaker_metadata.tsv:   0%|          | 0.00/9.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132c13b0f3b346a293d72145aa8e0f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker metadata loaded: (52, 6)\n",
      "Columns: ['speaker_id', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "  speaker_id  gender    age  \\\n",
      "0     KES001  Female  30-40   \n",
      "1     KES002  Female  30-40   \n",
      "2     KES003    Male  25-30   \n",
      "3     KES004    Male  25-30   \n",
      "4     KES005    Male  18-24   \n",
      "\n",
      "                          severity_speech_impairment  \\\n",
      "0                       Severe (frequent breakdowns)   \n",
      "1                       Severe (frequent breakdowns)   \n",
      "2  Profound (communication very difficult or impo...   \n",
      "3                       Severe (frequent breakdowns)   \n",
      "4           Moderate (requires effort to understand)   \n",
      "\n",
      "             type_nonstandard_speech               etiology  \n",
      "0                         Dysarthria         Cerebral Palsy  \n",
      "1                         Dysarthria         Cerebral Palsy  \n",
      "2  Stuttering (Disfluency Disorders)         Cerebral Palsy  \n",
      "3  Stuttering (Disfluency Disorders)  Neurological disorder  \n",
      "4  Stuttering (Disfluency Disorders)  Neurological disorder  \n",
      "Generating predictions...\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5859\n",
      "Adjusted CER: 0.2148\n",
      "LATTEScore: 6.25%\n",
      "Un-adjusted WER: 0.6644\n",
      "Un-adjusted CER: 0.2618\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4158\n",
      "Adjusted CER: 0.1528\n",
      "LATTEScore: 22.02%\n",
      "Un-adjusted WER: 0.4594\n",
      "Un-adjusted CER: 0.1701\n",
      "===============\n",
      "Dev WER: 0.737 | Word Accuracy: 26.3% | LATTEScore: 1.8%\n",
      "Test WER: 0.569 | Word Accuracy: 43.1% | LATTEScore: 11.6%\n",
      "\n",
      "Dev DataFrame shape: (272, 11)\n",
      "Columns: ['speaker_id', 'reference', 'prediction', 'wer', 'word_accuracy', 'lattescore_meaning_preserved', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "\n",
      "Test DataFrame shape: (554, 11)\n",
      "Columns: ['speaker_id', 'reference', 'prediction', 'wer', 'word_accuracy', 'lattescore_meaning_preserved', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "\n",
      "=== Model Deployment Analysis ===\n",
      "LATTEScore: 11.6%\n",
      "Deployment Threshold: 80.0%\n",
      "❌ RECOMMENDATION: Model does not meet quality standards\n",
      "   Consider: More training data, hyperparameter tuning, or different architecture\n",
      "\n",
      "=== FILES SAVED ===\n",
      "dev_predictions.csv: 272 samples\n",
      "test_predictions.csv: 554 samples\n",
      "\n",
      "=== DATA PREVIEW ===\n",
      "  speaker_id                                          reference  \\\n",
      "0     KES004  kila disemba familia yetu hukutana kijijini kw...   \n",
      "1     KES004  katika shule ya msingi ya langas ni ni ni nili...   \n",
      "2     KES004  nilipofanya safari ya kitalii katika mbuga ya ...   \n",
      "3     KES004  kila jumapili mama huamka mapena na kutengenez...   \n",
      "4     KES004  nilipokua chuo kikuu nairobi tulikua na na uta...   \n",
      "5     KES004  katika kampaini za usafi mtaani kibera sisi ka...   \n",
      "6     KES004  nilipokuwa soko la gikomba nilikutana na mama ...   \n",
      "7     KES004  siku moja nilisaidia shangazi yangu kuvuna mah...   \n",
      "8     KES004  nilihudhuria harusi ya ndugu yangu katika kaun...   \n",
      "9     KES004  katika safari yangu ya ya kwenda kakamega tuli...   \n",
      "\n",
      "                                          prediction       wer  \\\n",
      "0   Kila didisema familia yetu ukutana kijijini k...  0.500000   \n",
      "1   Katika shule ya msingi ya lelidanga nilijifun...  0.405405   \n",
      "2   Nilipofanya safari ya chitalika katika mbuga ...  0.409091   \n",
      "3   Kila jumapili mama wamuka mapema na kutengenz...  0.565217   \n",
      "4   Nilipokuwa chukitu huu Nairobi tulikona na ut...  0.555556   \n",
      "5   Katika kampeni za usafi mtani kimbera sisi ka...  0.375000   \n",
      "6   Nilipokuwa sokula jikombani nilikuwa kitanya ...  0.633333   \n",
      "7   Siku mmoja ni nisaidia shangazi yangu kufuna ...  0.593750   \n",
      "8   Niliudhuria arusi ya ndugu yangu katika count...  0.531250   \n",
      "9   Kiatika safari ya konda kamega tulisemama kwe...  0.437500   \n",
      "\n",
      "   lattescore_meaning_preserved  \n",
      "0                             0  \n",
      "1                             0  \n",
      "2                             0  \n",
      "3                             0  \n",
      "4                             0  \n",
      "5                             0  \n",
      "6                             0  \n",
      "7                             0  \n",
      "8                             0  \n",
      "9                             0  \n",
      "\n",
      "=== DOWNLOAD LINKS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='dev_predictions.csv' target='_blank'>dev_predictions.csv</a><br>"
      ],
      "text/plain": [
       "/nairobo_innovation_sprint/dev_predictions.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='test_predictions.csv' target='_blank'>test_predictions.csv</a><br>"
      ],
      "text/plain": [
       "/nairobo_innovation_sprint/test_predictions.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEXT STEPS ===\n",
      "1. Analyze LATTEScore by speaker metadata (etiology, severity, gender)\n",
      "2. Compare LATTEScore with WER to see if meaning preservation differs from word accuracy\n",
      "3. Use LATTEScore for model deployment decisions\n",
      "4. Calculate LATTEScore breakdown by speaker characteristics\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import FileLink, display\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Load external speaker metadata directly ---\n",
    "metadata_url = \"https://huggingface.co/datasets/cdli/kenyan_swahili_nonstandard_speech_v0.9/resolve/main/speaker_metadata.tsv\"\n",
    "speaker_metadata_ds = load_dataset(\"csv\", data_files=metadata_url, sep=\"\\t\")[\"train\"]\n",
    "speaker_metadata = pd.DataFrame(speaker_metadata_ds)\n",
    "\n",
    "# Drop unwanted columns\n",
    "speaker_metadata = speaker_metadata.drop(columns=[\"comments\", \"slp_id\"], errors=\"ignore\")\n",
    "\n",
    "print(\"Speaker metadata loaded:\", speaker_metadata.shape)\n",
    "print(\"Columns:\", speaker_metadata.columns.tolist())\n",
    "print(speaker_metadata.head())\n",
    "\n",
    "# --- Function to calculate individual WER/accuracy ---\n",
    "def calculate_individual_wer(prediction, reference):\n",
    "    return wer_metric.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "# --- LATTEScore calculation function ---\n",
    "def calculate_lattescore(prediction, reference, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Calculate whether meaning is preserved for a single transcript pair\n",
    "    Returns 1 if meaning preserved, 0 if meaning lost\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Skip empty strings\n",
    "        if not reference.strip() or not prediction.strip():\n",
    "            return 0\n",
    "            \n",
    "        # Get sentence embeddings\n",
    "        emb_ref = model.encode(reference, convert_to_tensor=True)\n",
    "        emb_pred = model.encode(prediction, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(emb_ref, emb_pred).item()\n",
    "        \n",
    "        # Consider meaning preserved if similarity > threshold\n",
    "        return 1 if similarity > similarity_threshold else 0\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: use WER-based approximation\n",
    "        wer = wer_metric.compute(predictions=[prediction], references=[reference])\n",
    "        # Conservative threshold: meaning preserved if WER < 0.3 (30%)\n",
    "        return 1 if wer < 0.3 else 0\n",
    "\n",
    "# --- You need to get predictions first! Add this: ---\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Generate predictions for dev set\n",
    "preds_dev = trainer.predict(dev_dataset)\n",
    "dev_predictions = processor.tokenizer.batch_decode(preds_dev.predictions, skip_special_tokens=True)\n",
    "dev_references = [transcript_normalizer(x) for x in dev_dataset[\"transcription\"]]\n",
    "\n",
    "# Generate predictions for test set  \n",
    "preds_test = trainer.predict(test_dataset)\n",
    "test_predictions = processor.tokenizer.batch_decode(preds_test.predictions, skip_special_tokens=True)\n",
    "test_references = [transcript_normalizer(x) for x in test_dataset[\"transcription\"]]\n",
    "\n",
    "# Create prediction dictionaries\n",
    "preds_dev_dict = {\n",
    "    \"speaker_id\": dev_dataset[\"speaker_id\"],\n",
    "    \"transcription\": dev_references,\n",
    "    \"prediction\": dev_predictions\n",
    "}\n",
    "\n",
    "preds_test_dict = {\n",
    "    \"speaker_id\": test_dataset[\"speaker_id\"], \n",
    "    \"transcription\": test_references,\n",
    "    \"prediction\": test_predictions\n",
    "}\n",
    "\n",
    "# Calculate WER for entire sets\n",
    "wer_dev = wer_metric.compute(predictions=dev_predictions, references=dev_references)\n",
    "wer_test = wer_metric.compute(predictions=test_predictions, references=test_references)\n",
    "\n",
    "# Calculate metrics for individual examples\n",
    "dev_wer_individual = [calculate_individual_wer(p, r) for p, r in zip(dev_predictions, dev_references)]\n",
    "dev_acc_individual = [(1 - wer) * 100 for wer in dev_wer_individual]\n",
    "dev_lattescore_individual = [calculate_lattescore(p, r) for p, r in zip(dev_predictions, dev_references)]\n",
    "\n",
    "test_wer_individual = [calculate_individual_wer(p, r) for p, r in zip(test_predictions, test_references)]\n",
    "test_acc_individual = [(1 - wer) * 100 for wer in test_wer_individual]\n",
    "test_lattescore_individual = [calculate_lattescore(p, r) for p, r in zip(test_predictions, test_references)]\n",
    "\n",
    "# Calculate overall LATTEScore percentages\n",
    "dev_lattescore_percent = (sum(dev_lattescore_individual) / len(dev_lattescore_individual)) * 100\n",
    "test_lattescore_percent = (sum(test_lattescore_individual) / len(test_lattescore_individual)) * 100\n",
    "\n",
    "acc_dev = (1 - wer_dev) * 100\n",
    "acc_test = (1 - wer_test) * 100\n",
    "\n",
    "print(f\"Dev WER: {wer_dev:.3f} | Word Accuracy: {acc_dev:.1f}% | LATTEScore: {dev_lattescore_percent:.1f}%\")\n",
    "print(f\"Test WER: {wer_test:.3f} | Word Accuracy: {acc_test:.1f}% | LATTEScore: {test_lattescore_percent:.1f}%\")\n",
    "\n",
    "# --- Enhanced DataFrame creation with metadata merge ---\n",
    "def create_enhanced_dataframe(dataset, wer_individual, acc_individual, lattescore_individual, dataset_name):\n",
    "    base_data = {\n",
    "        \"speaker_id\": dataset[\"speaker_id\"],\n",
    "        \"reference\": dataset[\"transcription\"],\n",
    "        \"prediction\": dataset[\"prediction\"],\n",
    "        \"wer\": wer_individual,\n",
    "        \"word_accuracy\": acc_individual,\n",
    "        \"lattescore_meaning_preserved\": lattescore_individual,  # 1=preserved, 0=lost\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(base_data)\n",
    "    df = df.merge(speaker_metadata, on=\"speaker_id\", how=\"left\")\n",
    "\n",
    "    print(f\"\\n{dataset_name} DataFrame shape: {df.shape}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    return df\n",
    "\n",
    "# Build enhanced DataFrames (THIS WAS MISSING!)\n",
    "df_dev_enhanced = create_enhanced_dataframe(preds_dev_dict, dev_wer_individual, dev_acc_individual, dev_lattescore_individual, \"Dev\")\n",
    "df_test_enhanced = create_enhanced_dataframe(preds_test_dict, test_wer_individual, test_acc_individual, test_lattescore_individual, \"Test\")\n",
    "\n",
    "# --- Model deployment analysis using LATTEScore ---\n",
    "def analyze_model_deployment(lattescore, threshold=80.0):\n",
    "    \"\"\"Analyze if model meets quality standards based on LATTEScore\"\"\"\n",
    "    print(f\"\\n=== Model Deployment Analysis ===\")\n",
    "    print(f\"LATTEScore: {lattescore:.1f}%\")\n",
    "    print(f\"Deployment Threshold: {threshold}%\")\n",
    "    \n",
    "    if lattescore >= threshold:\n",
    "        print(\"✅ RECOMMENDATION: Model meets quality standards for deployment\")\n",
    "        print(\"   The ASR model preserves meaning in most transcripts\")\n",
    "    else:\n",
    "        print(\"❌ RECOMMENDATION: Model does not meet quality standards\")\n",
    "        print(\"   Consider: More training data, hyperparameter tuning, or different architecture\")\n",
    "    \n",
    "    return lattescore >= threshold\n",
    "\n",
    "# Run deployment analysis\n",
    "deployment_ready = analyze_model_deployment(test_lattescore_percent)\n",
    "\n",
    "# --- Save only the main analysis files ---\n",
    "df_dev_enhanced.to_csv(\"dev_predictions.csv\", index=False)\n",
    "df_test_enhanced.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== FILES SAVED ===\")\n",
    "print(f\"dev_predictions.csv: {len(df_dev_enhanced)} samples\")\n",
    "print(f\"test_predictions.csv: {len(df_test_enhanced)} samples\")\n",
    "\n",
    "# --- Preview data ---\n",
    "print(\"\\n=== DATA PREVIEW ===\")\n",
    "print(df_dev_enhanced[['speaker_id', 'reference', 'prediction', 'wer', 'lattescore_meaning_preserved']].head(10))\n",
    "\n",
    "# --- Download links ---\n",
    "print(\"\\n=== DOWNLOAD LINKS ===\")\n",
    "display(FileLink(\"dev_predictions.csv\"))\n",
    "display(FileLink(\"test_predictions.csv\"))\n",
    "\n",
    "# --- Updated next steps ---\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Analyze LATTEScore by speaker metadata (etiology, severity, gender)\")\n",
    "print(\"2. Compare LATTEScore with WER to see if meaning preservation differs from word accuracy\")\n",
    "print(\"3. Use LATTEScore for model deployment decisions\")\n",
    "print(\"4. Calculate LATTEScore breakdown by speaker characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9082ef",
   "metadata": {},
   "source": [
    "## Store Model\n",
    "\n",
    "--> save best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d4454",
   "metadata": {},
   "source": [
    "### Save to your volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ff87bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_large_4/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with \"load_best_model_at_end=True\" set in the settings (this is the default, so don't change that), after training is completed the best model is loaded and then saved\n",
    "best_model_dir = os.path.join(OUTPUT_DIR, 'best_model')\n",
    "print(f\"Saving to: {best_model_dir}\")\n",
    "trainer.model.save_pretrained(best_model_dir, safe_serialization=True)\n",
    "trainer.tokenizer.save_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d0a44-f80a-4325-9844-2cce4c8e9e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
