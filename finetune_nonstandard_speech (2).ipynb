{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21c1499",
   "metadata": {},
   "source": [
    "# Fine-tune Whisper on CDLI Non-Standard Speech Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43222608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 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\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "HF_TOKEN = input()\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f87c76",
   "metadata": {},
   "source": [
    "## Settings \n",
    "\n",
    "--> adapt for your scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3ce04",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "95aac5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will write model to: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_small_2\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# storage in Volume that will persist\n",
    "LOCAL_STORAGE_DIR = '/jupyter_kernel'\n",
    "\n",
    "BASE_DIR = os.path.join(LOCAL_STORAGE_DIR, 'trained_models')\n",
    "!mkdir -p {BASE_DIR}\n",
    "\n",
    "# directory for model training\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'sw_nonstandard_tune_whisper_small_2')\n",
    "#OUTPUT_DIR = os.path.join(BASE_DIR, 'en_nonstandard_tune_whisper_large_1')\n",
    "\n",
    "print(f\"Will write model to: {OUTPUT_DIR}\")\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    raise ValueError(f\"Output directory already exists - if you continue this will overwrite data and may lead to strange results...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341cb64",
   "metadata": {},
   "source": [
    "### Model and Dataset settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0091bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#WHISPER_MODEL_TYPE = \"openai/whisper-tiny\" \n",
    "WHISPER_MODEL_TYPE = \"openai/whisper-small\" \n",
    "# WHISPER_MODEL_TYPE = \"openai/whisper-large-v3\" \n",
    "\n",
    "#LANGUAGE = 'en'\n",
    "#DATASET_NAME = \"cdli/kenyan_english_nonstandard_speech_v0.9\"\n",
    "\n",
    "LANGUAGE = 'sw'\n",
    "DATASET_NAME = \"cdli/kenyan_swahili_nonstandard_speech_v0.9\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c808490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which parts of the model to update\n",
    "UPDATE_ENCODER = True\n",
    "UPDATE_PROJ = True\n",
    "UPDATE_DECODER = False\n",
    "\n",
    "# Turn on SpecAugment\n",
    "USE_SPECAUGMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf2f146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model will be loaded from: openai/whisper-small\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######################\n",
    "## don't change these!\n",
    "######################\n",
    "\n",
    "\n",
    "TASK = \"transcribe\"\n",
    "\n",
    "BASE_MODEL_NAME = WHISPER_MODEL_TYPE\n",
    "print('Base model will be loaded from:', BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2735de",
   "metadata": {},
   "source": [
    "### Trainer Settings\n",
    "\n",
    "--> adjust as needed or keep defaults (these settings should be a good starting point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "104baa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING_STEPS = 10  # Increased slightly since training will be longer\n",
    "# if save steps is 0, only last and best model will be written\n",
    "SAVE_STEPS = 100    # Increased to reduce checkpoint frequency\n",
    "\n",
    "# training duration\n",
    "MAX_EPOCHS = 8      # Reduced from 10 for more reasonable training time\n",
    "MAX_STEPS = 600     # Reduced from 1000 - sufficient for small model\n",
    "\n",
    "# Learning Rate and LR Scheduler (LR_END and LR_DECAY_POWER only apply to polynomial)\n",
    "LEARNING_RATE = 1e-4 #@param - Good for small model\n",
    "LR_SCHEDULER_TYPE = 'polynomial' # constant_with_warmup or polynomial\n",
    "LR_WARMUP_STEPS = 50\n",
    "LR_END = 1e-8\n",
    "LR_DECAY_POWER = 4\n",
    "\n",
    "BATCH_SIZE = 12     # Reduced from 32 - small model needs smaller batches\n",
    "EVAL_BATCH_SIZE = 8 # Reduced from 16\n",
    "\n",
    "#@markdown other settings relevant for evaluation\n",
    "MAX_GEN_LEN = 128 # increase if your data has long sequences!\n",
    "EVAL_ON_START = True\n",
    "EVAL_STEPS = 50    # Good frequency for monitoring\n",
    "\n",
    "# for CPU, set both to false\n",
    "USE_FP16 = True    # Keep enabled for memory efficiency\n",
    "USE_BF16 = False   # only some GPUs support this, eg A100, A40\n",
    "\n",
    "# checkpoints get huge for large models (~18 GB!)\n",
    "NUM_CHECKPOINTS_TO_STORE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57078735",
   "metadata": {},
   "source": [
    "## Imports and Prep\n",
    "\n",
    "--> no need to change anything here, just run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "effd0080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: False\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# more efficient dataset handling\n",
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()\n",
    "\n",
    "# check if we have gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7034f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import random\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "\n",
    "import tarfile\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import evaluate\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "transcript_normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "903eac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "290a30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wer(references, predictions, normalize=True, verbose=True):\n",
    "  rs = references\n",
    "  ps = predictions\n",
    "  if normalize:\n",
    "    ps = [transcript_normalizer(x) for x in predictions]\n",
    "    rs = [transcript_normalizer(x) for x in references]\n",
    "  if verbose:\n",
    "    for r, p in zip(rs, ps):\n",
    "      print(r)\n",
    "      print(p)\n",
    "      print()\n",
    "\n",
    "  return wer_metric.compute(references=rs, predictions=ps)\n",
    "\n",
    "def compute_lattescore(references, predictions, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    LATTEScore calculation - percentage of transcripts that preserve meaning\n",
    "    Based on the paper: Large Language Models As A Proxy For Human Evaluation\n",
    "    This is a simplified version using semantic similarity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to use sentence transformers for better semantic similarity\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        preserved_count = 0\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            # Skip empty strings\n",
    "            if not ref.strip() or not pred.strip():\n",
    "                continue\n",
    "                \n",
    "            # Get sentence embeddings\n",
    "            emb_ref = model.encode(ref, convert_to_tensor=True)\n",
    "            emb_pred = model.encode(pred, convert_to_tensor=True)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = util.pytorch_cos_sim(emb_ref, emb_pred).item()\n",
    "            \n",
    "            # Consider meaning preserved if similarity > threshold\n",
    "            if similarity > similarity_threshold:\n",
    "                preserved_count += 1\n",
    "        \n",
    "        total_count = len([r for r in references if r.strip()])\n",
    "        lattescore = (preserved_count / total_count) * 100 if total_count > 0 else 0\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: use WER-based approximation if sentence-transformers not available\n",
    "        print(\"Sentence transformers not available, using WER-based LATTEScore approximation\")\n",
    "        preserved_count = 0\n",
    "        for ref, pred in zip(references, predictions):\n",
    "            # Skip empty strings\n",
    "            if not ref.strip() or not pred.strip():\n",
    "                continue\n",
    "                \n",
    "            wer = wer_metric.compute(predictions=[pred], references=[ref])\n",
    "            # Conservative threshold: meaning preserved if WER < 0.3 (30%)\n",
    "            if wer < 0.3:\n",
    "                preserved_count += 1\n",
    "        \n",
    "        total_count = len([r for r in references if r.strip()])\n",
    "        lattescore = (preserved_count / total_count) * 100 if total_count > 0 else 0\n",
    "    \n",
    "    return lattescore\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for ASR evaluation including WER, CER, and LATTEScore\n",
    "    \"\"\"\n",
    "    # for training metrics\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_strs = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_strs = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # calculate a per-example average for WER and CER\n",
    "    wers = []\n",
    "    cers = []\n",
    "    for pred_str, label_str in zip(pred_strs, label_strs):\n",
    "        p = transcript_normalizer(pred_str)\n",
    "        l = transcript_normalizer(label_str)\n",
    "        # Skip empty strings for metric calculation\n",
    "        if l.strip() and p.strip():\n",
    "            wer = wer_metric.compute(predictions=[p], references=[l])\n",
    "            cer = cer_metric.compute(predictions=[p], references=[l])\n",
    "            wers.append(wer)\n",
    "            cers.append(cer)\n",
    "\n",
    "    wer = np.mean([min(1.0, x) for x in wers]) if wers else 1.0\n",
    "    cer = np.mean([min(1.0, x) for x in cers]) if cers else 1.0\n",
    "    \n",
    "    # Calculate LATTEScore\n",
    "    lattescore = compute_lattescore(label_strs, pred_strs)\n",
    "    \n",
    "    print('=== Metrics ===')\n",
    "    print(f'Adjusted WER: {wer:.4f}')\n",
    "    print(f'Adjusted CER: {cer:.4f}')\n",
    "    print(f'LATTEScore: {lattescore:.2f}%')\n",
    "    print(f'Un-adjusted WER: {np.mean(wers) if wers else 1.0:.4f}')\n",
    "    print(f'Un-adjusted CER: {np.mean(cers) if cers else 1.0:.4f}')\n",
    "    print('===============')\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer, \n",
    "        \"cer\": cer, \n",
    "        \"lattescore\": lattescore\n",
    "    }\n",
    "\n",
    "# Optional: Add a function to analyze model quality based on LATTEScore\n",
    "def analyze_model_deployment(lattescore, threshold=80.0):\n",
    "    \"\"\"\n",
    "    Analyze if model meets quality standards for deployment based on LATTEScore\n",
    "    Using the 80% threshold from the research paper\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Model Deployment Analysis ===\")\n",
    "    print(f\"LATTEScore: {lattescore:.2f}%\")\n",
    "    print(f\"Deployment Threshold: {threshold}%\")\n",
    "    \n",
    "    if lattescore >= threshold:\n",
    "        print(\"âœ… RECOMMENDATION: Model meets quality standards for deployment\")\n",
    "        print(\"   The ASR model preserves meaning in most transcripts\")\n",
    "    else:\n",
    "        print(\"âŒ RECOMMENDATION: Model does not meet quality standards\")\n",
    "        print(\"   Consider: More training data, hyperparameter tuning, or different architecture\")\n",
    "    \n",
    "    return lattescore >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "105389f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name, split='test', limit_to_30_seconds=True):\n",
    "    \"\"\"\n",
    "    Load a dataset from Hugging Face Hub.\n",
    "    If limit_to_30_seconds is True, will only load examples with audio length <= 30 seconds.\n",
    "    \"\"\"\n",
    "    if split not in ['train', 'test', 'validation']:\n",
    "        raise ValueError(\"split must be one of 'train', 'test', or 'validation'\")\n",
    "    ds = datasets.load_dataset(dataset_name, split=split, streaming=False)\n",
    "    orig_len = len(ds)\n",
    "    if limit_to_30_seconds:\n",
    "        ds = ds.filter(lambda example: example['audio_length'] <= 30)\n",
    "        print(f\"Filtered dataset from {orig_len} to {len(ds)} examples with audio length <= 30 seconds\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac36c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following warning can be ignored:\n",
    "# \"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
    "# See: https://discuss.huggingface.co/t/finetuning-whisper-attention-mask-not-set-and-canot-be-inferred/97456\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb8215",
   "metadata": {},
   "source": [
    "## Download datasets and prepare features\n",
    "\n",
    "--> no need to change anything here, just run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcf0e7",
   "metadata": {},
   "source": [
    "### Optimizing some settings for dataset access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c69a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: False\n",
      "device is:  cuda\n",
      "# processors: 24\n"
     ]
    }
   ],
   "source": [
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is: ', device)\n",
    "\n",
    "# IMPORTANT! need to set to 1 to avoid the mapping to hang!\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()\n",
    "\n",
    "num_proc = min(32, os.cpu_count())\n",
    "print('# processors:', num_proc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a27923",
   "metadata": {},
   "source": [
    "### Load feature extractor\n",
    "\n",
    "--> for the model type you specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85919d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Language:  sw\n",
      "Using model: openai/whisper-small\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load processor\n",
    "print('Using Language: ', LANGUAGE)\n",
    "print('Using model:', WHISPER_MODEL_TYPE)\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_TYPE, language=LANGUAGE, task=TASK)\n",
    "\n",
    "# since this tokenizer isn't a FastTokenizer, so there is no point in running it with is_batched=True\n",
    "# see: processor.tokenizer.is_fast\n",
    "def prepare_features(example):\n",
    "    example[\"input_features\"] = processor.feature_extractor(example[\"audio\"][\"array\"], sampling_rate=example[\"audio\"][\"sampling_rate\"]).input_features[0]\n",
    "    example[\"labels\"] = processor.tokenizer(example[\"transcription\"]).input_ids\n",
    "    # also count number of tokens\n",
    "    example[\"token_length\"] = len(example[\"labels\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d19cd",
   "metadata": {},
   "source": [
    "### Load non-standard speech dataset\n",
    "\n",
    "We need to filter to 30 seconds, as Whisper can only train on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a020684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066e8cd3412b4871a492a759cdb77f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ed42ea5eaf4796bd6ac495df3f2e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00002.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb626570b3f40e5afec8dd588357621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00001-of-00002.parquet:   0%|          | 0.00/313M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5750bc305ab145b9b83c65cce4d464d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/338M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0334a38e73642ac9f409c8277a4f59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00006.parquet:   0%|          | 0.00/372M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edca4837542044599bbd9c010a0bb308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00006.parquet:   0%|          | 0.00/506M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c200a16fd514b4796cb270fbc84d4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00006.parquet:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be7443b91054f59ade837216833e429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00006.parquet:   0%|          | 0.00/509M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bafbcd347a24ffa8c964d78ba79ff02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00006.parquet:   0%|          | 0.00/567M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d80c749813419085bc49f99b6a3dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00006.parquet:   0%|          | 0.00/509M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffba0cf1b1024048ae6103823ca7a4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb8010742824577965f2d9f3176f49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329cc0aab5184b50b719eec080579e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929137b1a7d44e60b2f62059093be0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 3949 to 2856 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03a7b27bfeb4773b3dee47a18dfb0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/2856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TRAIN dataset with 2856 examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(DATASET_NAME, split='train', limit_to_30_seconds=True)\n",
    "train_dataset = train_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded TRAIN dataset with {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3a95a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61d5842e6f54a0d82c8ef7a5a3e09b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 865 to 554 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67de711566664b3a81ce11726fec7715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TEST dataset with 554 examples\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(DATASET_NAME, split='test', limit_to_30_seconds=True)\n",
    "test_dataset = test_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded TEST dataset with {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c34fa18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e294e99f84e4217a8f44865f65c48f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset from 417 to 272 examples with audio length <= 30 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8df9d0c09249ccadf57f8d0ec71d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DEV dataset with 272 examples\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = load_dataset(DATASET_NAME, split='validation', limit_to_30_seconds=True)\n",
    "dev_dataset = dev_dataset.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "print(f\"Loaded DEV dataset with {len(dev_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d1182",
   "metadata": {},
   "source": [
    "## Prepare Trainer\n",
    "\n",
    "--> no need to change anything here, just run\n",
    "\n",
    "Whenever something is changed in the settings, you need to rerun this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b402d733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Language:  sw\n",
      "Using model: openai/whisper-small\n",
      "language set to: sw\n"
     ]
    }
   ],
   "source": [
    "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
    "_ = base_model.to(device)\n",
    "print('Using Language: ', LANGUAGE)\n",
    "print('Using model:', WHISPER_MODEL_TYPE)\n",
    "\n",
    "# ensure task and language for training\n",
    "base_model.generation_config.language = LANGUAGE\n",
    "base_model.generation_config.task = TASK\n",
    "base_model.generation_config.forced_decoder_ids = None\n",
    "base_model.config.forced_decoder_ids = None\n",
    "# to use gradient checkpointing\n",
    "base_model.config.use_cache = False\n",
    "print('language set to:', base_model.generation_config.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f46b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using specaugment: True\n",
      "Using audio augmentation: True\n",
      "Audio augmentation probability: 0.7\n",
      "Using specaugment: True\n",
      "=== Using BOTH augmentation types: SpecAugment (feature-level) + Audio (signal-level) ===\n",
      "Audio augmentation functions defined. Use prepare_train_features for training data and prepare_eval_features for test/dev data.\n"
     ]
    }
   ],
   "source": [
    "# Add SpecAugment\n",
    "if USE_SPECAUGMENT:\n",
    "    base_model.config.apply_spec_augment = USE_SPECAUGMENT\n",
    "\n",
    "    # Specaugment (use default settings, as per paper)\n",
    "    # time masking\n",
    "    base_model.config.mask_time_prob = 0.05\n",
    "    base_model.config.mask_time_length = 10\n",
    "    base_model.config.mask_time_min_masks = 2\n",
    "\n",
    "    # feature masking\n",
    "    base_model.config.mask_feature_prob = 0.05 # def: 0\n",
    "    base_model.config.mask_feature_length = 10\n",
    "    base_model.config.mask_feature_min_masks = 2 # def: 0\n",
    "\n",
    "print('Using specaugment:', base_model.config.apply_spec_augment)\n",
    "\n",
    "# Add Audio Augmentation functions\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "def apply_audio_augmentation(audio_array, sample_rate=16000, augmentation_prob=0.7):\n",
    "    \"\"\"\n",
    "    Apply audio augmentations to training data\n",
    "    augmentation_prob: probability of applying any augmentation to an example\n",
    "    \"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return audio_array  # No augmentation applied\n",
    "    \n",
    "    # Convert to tensor for torchaudio transforms\n",
    "    audio_tensor = torch.from_numpy(audio_array).float()\n",
    "    original_length = len(audio_tensor)\n",
    "    \n",
    "    # Track which augmentations were applied for debugging\n",
    "    applied_augmentations = []\n",
    "    \n",
    "    # 1. Add background noise (30% chance if augmenting)\n",
    "    if np.random.random() < 0.3:\n",
    "        noise_level = np.random.uniform(0.001, 0.01)\n",
    "        noise = torch.randn_like(audio_tensor) * noise_level\n",
    "        audio_tensor = audio_tensor + noise\n",
    "        applied_augmentations.append(f\"noise({noise_level:.3f})\")\n",
    "    \n",
    "    # 2. Time masking (25% chance if augmenting)\n",
    "    if np.random.random() < 0.25 and original_length > 1000:\n",
    "        mask_length = np.random.randint(50, 300)\n",
    "        mask_start = np.random.randint(0, max(1, original_length - mask_length))\n",
    "        audio_tensor[mask_start:mask_start + mask_length] = 0\n",
    "        applied_augmentations.append(f\"time_mask({mask_length})\")\n",
    "    \n",
    "    # 3. Pitch shift (20% chance if augmenting)\n",
    "    if np.random.random() < 0.2:\n",
    "        try:\n",
    "            n_steps = np.random.choice([-2, -1, 1, 2])\n",
    "            pitch_shift = T.PitchShift(sample_rate, n_steps=n_steps)\n",
    "            audio_tensor = pitch_shift(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"pitch({n_steps})\")\n",
    "        except Exception as e:\n",
    "            # Fallback to simple resampling if pitch shift fails\n",
    "            try:\n",
    "                speed_factor = 1.0 + (n_steps * 0.1)\n",
    "                new_length = int(original_length / speed_factor)\n",
    "                audio_tensor = torch.nn.functional.interpolate(\n",
    "                    audio_tensor.unsqueeze(0).unsqueeze(0), \n",
    "                    size=new_length, \n",
    "                    mode='linear'\n",
    "                ).squeeze(0).squeeze(0)\n",
    "                if len(audio_tensor) > original_length:\n",
    "                    audio_tensor = audio_tensor[:original_length]\n",
    "                else:\n",
    "                    audio_tensor = torch.nn.functional.pad(\n",
    "                        audio_tensor, \n",
    "                        (0, original_length - len(audio_tensor))\n",
    "                    )\n",
    "                applied_augmentations.append(f\"pitch_approx({n_steps})\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 4. Time stretching - speed up/slow down (25% chance if augmenting)\n",
    "    if np.random.random() < 0.25 and original_length > 500:\n",
    "        rate = np.random.uniform(0.85, 1.15)  # 15% speed variation\n",
    "        new_length = int(original_length * rate)\n",
    "        \n",
    "        if new_length > 100:  # Ensure reasonable length\n",
    "            try:\n",
    "                # High-quality time stretching using interpolation\n",
    "                audio_tensor_stretched = torch.nn.functional.interpolate(\n",
    "                    audio_tensor.unsqueeze(0).unsqueeze(0), \n",
    "                    size=new_length, \n",
    "                    mode='linear',\n",
    "                    align_corners=False\n",
    "                ).squeeze(0).squeeze(0)\n",
    "                \n",
    "                # Trim or pad to original length to maintain consistency\n",
    "                if len(audio_tensor_stretched) > original_length:\n",
    "                    audio_tensor = audio_tensor_stretched[:original_length]\n",
    "                else:\n",
    "                    padding = torch.zeros(original_length - len(audio_tensor_stretched))\n",
    "                    audio_tensor = torch.cat([audio_tensor_stretched, padding])\n",
    "                \n",
    "                applied_augmentations.append(f\"time_stretch({rate:.2f})\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    # 5. Volume change (35% chance if augmenting)\n",
    "    if np.random.random() < 0.35:\n",
    "        gain = np.random.uniform(0.5, 1.5)\n",
    "        audio_tensor = audio_tensor * gain\n",
    "        # Clip to prevent distortion\n",
    "        audio_tensor = torch.clamp(audio_tensor, -1.0, 1.0)\n",
    "        applied_augmentations.append(f\"volume({gain:.2f})\")\n",
    "    \n",
    "    # 6. Low-pass filter - room simulation (15% chance if augmenting)\n",
    "    if np.random.random() < 0.15 and original_length > 1000:\n",
    "        try:\n",
    "            cutoff_freq = np.random.uniform(2000, 4000)  # Simulate telephone quality\n",
    "            lowpass = T.LowpassBiquad(sample_rate, cutoff_freq=cutoff_freq)\n",
    "            audio_tensor = lowpass(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"lowpass({cutoff_freq:.0f}Hz)\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # 7. High-pass filter (10% chance if augmenting) - remove low frequencies\n",
    "    if np.random.random() < 0.1 and original_length > 1000:\n",
    "        try:\n",
    "            cutoff_freq = np.random.uniform(100, 500)\n",
    "            highpass = T.HighpassBiquad(sample_rate, cutoff_freq=cutoff_freq)\n",
    "            audio_tensor = highpass(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "            applied_augmentations.append(f\"highpass({cutoff_freq:.0f}Hz)\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Debug: print augmentations for first few examples\n",
    "    if len(applied_augmentations) > 0 and np.random.random() < 0.01:  # 1% of augmented examples\n",
    "        print(f\"Applied augmentations: {', '.join(applied_augmentations)}\")\n",
    "    \n",
    "    return audio_tensor.numpy()\n",
    "\n",
    "# Audio augmentation settings (add these to your settings cell)\n",
    "USE_AUDIO_AUGMENTATION = True\n",
    "AUGMENTATION_PROB = 0.7  # 70% of training examples get augmentation\n",
    "\n",
    "print('Using audio augmentation:', USE_AUDIO_AUGMENTATION)\n",
    "print('Audio augmentation probability:', AUGMENTATION_PROB)\n",
    "print('Using specaugment:', base_model.config.apply_spec_augment)\n",
    "print('=== Using BOTH augmentation types: SpecAugment (feature-level) + Audio (signal-level) ===')\n",
    "\n",
    "# Modified prepare_features function with augmentation support\n",
    "def prepare_features(example, is_training=True):\n",
    "    \"\"\"\n",
    "    Prepare features with optional augmentation for training data\n",
    "    \"\"\"\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sample_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    # Apply augmentation ONLY to training data\n",
    "    if is_training and USE_AUDIO_AUGMENTATION:\n",
    "        audio_array = apply_audio_augmentation(audio_array, sample_rate, AUGMENTATION_PROB)\n",
    "    \n",
    "    # Extract features as before\n",
    "    example[\"input_features\"] = processor.feature_extractor(\n",
    "        audio_array, \n",
    "        sampling_rate=sample_rate\n",
    "    ).input_features[0]\n",
    "    \n",
    "    example[\"labels\"] = processor.tokenizer(example[\"transcription\"]).input_ids\n",
    "    example[\"token_length\"] = len(example[\"labels\"])\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Create wrapper functions for dataset mapping\n",
    "def prepare_train_features(example):\n",
    "    return prepare_features(example, is_training=True)\n",
    "\n",
    "def prepare_eval_features(example):\n",
    "    return prepare_features(example, is_training=False)\n",
    "\n",
    "print(\"Audio augmentation functions defined. Use prepare_train_features for training data and prepare_eval_features for test/dev data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bf40332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating encoder: True\n",
      "Updating projection layer: True\n",
      "Updating decoder: False\n",
      "Overview to number of model parameters to be updated:\n",
      "* encoder params to update/total: 88154112 88154112\n",
      "* decoder parans to update/total: 39832320 153580800\n",
      "* overall # trainable parameters: 127986432\n",
      "*     overall # model parameters: 241734912\n"
     ]
    }
   ],
   "source": [
    "# which layers to tune\n",
    "\n",
    "print(\"Updating encoder:\", UPDATE_ENCODER)\n",
    "print(\"Updating projection layer:\", UPDATE_PROJ)\n",
    "print(\"Updating decoder:\", UPDATE_DECODER)\n",
    "\n",
    "\n",
    "base_model.model.encoder.requires_grad_(UPDATE_ENCODER)\n",
    "base_model.model.decoder.requires_grad_(UPDATE_DECODER)\n",
    "base_model.proj_out.requires_grad_(UPDATE_PROJ)\n",
    "\n",
    "print(\"Overview to number of model parameters to be updated:\")\n",
    "print('* encoder params to update/total:', count_trainable_parameters(base_model.model.encoder), base_model.model.encoder.num_parameters())\n",
    "print('* decoder parans to update/total:', count_trainable_parameters(base_model.model.decoder), base_model.model.decoder.num_parameters())\n",
    "\n",
    "print('* overall # trainable parameters:', count_trainable_parameters(base_model))\n",
    "print('*     overall # model parameters:', base_model.model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d40350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer args set, writing to: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_small_2\n"
     ]
    }
   ],
   "source": [
    "# Training Hyper Parameters\n",
    "# don't change settings here, but instead at very top!\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    include_num_input_tokens_seen=True,\n",
    "    ### on GPU, can either do fp16 or bf16 depending on specific GPU\n",
    "    fp16=USE_FP16, \n",
    "    bf16=USE_BF16, \n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    #\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    #\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    #\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    #\n",
    "    eval_on_start=EVAL_ON_START,\n",
    "    predict_with_generate=True,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    generation_max_length=MAX_GEN_LEN,\n",
    "    #\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    #\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    #\n",
    "    # only applies to polynomial schedule (constant ignores args)\n",
    "    lr_scheduler_kwargs={\n",
    "        \"lr_end\": LR_END, # The final LR.  Crucial for polynomial decay.\n",
    "        \"power\": LR_DECAY_POWER, # for decay\n",
    "        # we don't need to set the other arguments as they are already set in the args outside\n",
    "        #\"num_warmup_steps\": WARMUP_STEPS, # The number of steps for the warmup phase.\n",
    "        #\"num_training_steps\": MAX_STEPS, # The total number of training steps.\n",
    "        #\"lr_init\": 1e-5 # we take the LR setting\n",
    "    },\n",
    "\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=LR_WARMUP_STEPS, # what happens if we have this and the LR schedule args ?\n",
    "    #\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=NUM_CHECKPOINTS_TO_STORE,\n",
    "    load_best_model_at_end=True,\n",
    "    # group_by_length=True\n",
    "    # auto_find_batch_size=True\n",
    ")\n",
    "\n",
    "print('trainer args set, writing to:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39301594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking for sequences that are too long...\n",
      "Original dataset sizes:\n",
      "Train: 2856 examples\n",
      "Dev: 272 examples\n",
      "Test: 554 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ef34fe0a3146bf9dfb98d402c665ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Removing sequence with 617 tokens (max is 448)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b280111d264472aa9b8f7561e2ddc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d873a4a980948fda0818b49f6d871ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset sizes:\n",
      "Train: 2855 examples\n",
      "Dev: 272 examples\n",
      "Test: 554 examples\n",
      "Maximum sequence lengths after filtering:\n",
      "Train: 213 tokens\n",
      "Dev: 126 tokens\n",
      "Test: 153 tokens\n",
      "âœ… All sequences are within Whisper's maximum length limit!\n",
      "ðŸŽ¯ Ready to start training...\n",
      "model training dir: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_small_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 39:34, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Lattescore</th>\n",
       "      <th>Input Tokens Seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.943650</td>\n",
       "      <td>0.507359</td>\n",
       "      <td>0.219101</td>\n",
       "      <td>17.279412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.982675</td>\n",
       "      <td>0.511222</td>\n",
       "      <td>0.205066</td>\n",
       "      <td>16.911765</td>\n",
       "      <td>144000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>0.983821</td>\n",
       "      <td>0.519094</td>\n",
       "      <td>0.232393</td>\n",
       "      <td>18.382353</td>\n",
       "      <td>288000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.925288</td>\n",
       "      <td>0.490694</td>\n",
       "      <td>0.223873</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>432000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.831300</td>\n",
       "      <td>0.880293</td>\n",
       "      <td>0.455930</td>\n",
       "      <td>0.207017</td>\n",
       "      <td>27.573529</td>\n",
       "      <td>576000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.849920</td>\n",
       "      <td>0.438330</td>\n",
       "      <td>0.196733</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>719760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.544400</td>\n",
       "      <td>0.845099</td>\n",
       "      <td>0.428024</td>\n",
       "      <td>0.191377</td>\n",
       "      <td>31.985294</td>\n",
       "      <td>863760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.838860</td>\n",
       "      <td>0.426373</td>\n",
       "      <td>0.189715</td>\n",
       "      <td>32.352941</td>\n",
       "      <td>1007760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.566400</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.427506</td>\n",
       "      <td>0.193774</td>\n",
       "      <td>31.617647</td>\n",
       "      <td>1151760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.835407</td>\n",
       "      <td>0.426725</td>\n",
       "      <td>0.195732</td>\n",
       "      <td>32.720588</td>\n",
       "      <td>1295760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>0.834985</td>\n",
       "      <td>0.425341</td>\n",
       "      <td>0.192795</td>\n",
       "      <td>31.985294</td>\n",
       "      <td>1439520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.835074</td>\n",
       "      <td>0.425016</td>\n",
       "      <td>0.192825</td>\n",
       "      <td>31.985294</td>\n",
       "      <td>1583520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.625800</td>\n",
       "      <td>0.835121</td>\n",
       "      <td>0.425236</td>\n",
       "      <td>0.192855</td>\n",
       "      <td>31.985294</td>\n",
       "      <td>1727520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5074\n",
      "Adjusted CER: 0.2191\n",
      "LATTEScore: 17.28%\n",
      "Un-adjusted WER: 0.6787\n",
      "Un-adjusted CER: 0.3086\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5112\n",
      "Adjusted CER: 0.2051\n",
      "LATTEScore: 16.91%\n",
      "Un-adjusted WER: 0.5831\n",
      "Un-adjusted CER: 0.2444\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5191\n",
      "Adjusted CER: 0.2324\n",
      "LATTEScore: 18.38%\n",
      "Un-adjusted WER: 0.6821\n",
      "Un-adjusted CER: 0.3062\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4907\n",
      "Adjusted CER: 0.2239\n",
      "LATTEScore: 25.00%\n",
      "Un-adjusted WER: 0.6670\n",
      "Un-adjusted CER: 0.3226\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4559\n",
      "Adjusted CER: 0.2070\n",
      "LATTEScore: 27.57%\n",
      "Un-adjusted WER: 0.5790\n",
      "Un-adjusted CER: 0.2692\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4383\n",
      "Adjusted CER: 0.1967\n",
      "LATTEScore: 29.41%\n",
      "Un-adjusted WER: 0.5820\n",
      "Un-adjusted CER: 0.2655\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4280\n",
      "Adjusted CER: 0.1914\n",
      "LATTEScore: 31.99%\n",
      "Un-adjusted WER: 0.5798\n",
      "Un-adjusted CER: 0.2670\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4264\n",
      "Adjusted CER: 0.1897\n",
      "LATTEScore: 32.35%\n",
      "Un-adjusted WER: 0.5726\n",
      "Un-adjusted CER: 0.2595\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4275\n",
      "Adjusted CER: 0.1938\n",
      "LATTEScore: 31.62%\n",
      "Un-adjusted WER: 0.5731\n",
      "Un-adjusted CER: 0.2643\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4267\n",
      "Adjusted CER: 0.1957\n",
      "LATTEScore: 32.72%\n",
      "Un-adjusted WER: 0.5930\n",
      "Un-adjusted CER: 0.2721\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4253\n",
      "Adjusted CER: 0.1928\n",
      "LATTEScore: 31.99%\n",
      "Un-adjusted WER: 0.5730\n",
      "Un-adjusted CER: 0.2603\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4250\n",
      "Adjusted CER: 0.1928\n",
      "LATTEScore: 31.99%\n",
      "Un-adjusted WER: 0.5727\n",
      "Un-adjusted CER: 0.2604\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4252\n",
      "Adjusted CER: 0.1929\n",
      "LATTEScore: 31.99%\n",
      "Un-adjusted WER: 0.5729\n",
      "Un-adjusted CER: 0.2604\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.661701435248057, metrics={'train_runtime': 2490.2388, 'train_samples_per_second': 2.891, 'train_steps_per_second': 0.241, 'total_flos': 2.07723771150336e+18, 'train_loss': 0.661701435248057, 'epoch': 2.5210084033613445, 'num_input_tokens_seen': 1727520000})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=base_model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor\n",
    ")\n",
    "\n",
    "# === COMPLETE SEQUENCE LENGTH FILTERING CODE BLOCK ===\n",
    "print(\"ðŸ” Checking for sequences that are too long...\")\n",
    "\n",
    "def filter_long_sequences(example):\n",
    "    token_length = len(example[\"labels\"])\n",
    "    if token_length > 448:\n",
    "        print(f\"âš ï¸ Removing sequence with {token_length} tokens (max is 448)\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Apply filtering to all datasets\n",
    "print(f\"Original dataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset)} examples\")\n",
    "print(f\"Dev: {len(dev_dataset)} examples\") \n",
    "print(f\"Test: {len(test_dataset)} examples\")\n",
    "\n",
    "# Filter the datasets\n",
    "train_dataset = train_dataset.filter(filter_long_sequences)\n",
    "dev_dataset = dev_dataset.filter(filter_long_sequences)\n",
    "test_dataset = test_dataset.filter(filter_long_sequences)\n",
    "\n",
    "print(f\"Filtered dataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset)} examples\")\n",
    "print(f\"Dev: {len(dev_dataset)} examples\")\n",
    "print(f\"Test: {len(test_dataset)} examples\")\n",
    "\n",
    "# Update trainer with filtered datasets\n",
    "trainer.train_dataset = train_dataset\n",
    "trainer.eval_dataset = dev_dataset\n",
    "\n",
    "# Verify maximum lengths\n",
    "max_train_len = max([len(x[\"labels\"]) for x in train_dataset]) if train_dataset else 0\n",
    "max_dev_len = max([len(x[\"labels\"]) for x in dev_dataset]) if dev_dataset else 0\n",
    "max_test_len = max([len(x[\"labels\"]) for x in test_dataset]) if test_dataset else 0\n",
    "\n",
    "print(f\"Maximum sequence lengths after filtering:\")\n",
    "print(f\"Train: {max_train_len} tokens\")\n",
    "print(f\"Dev: {max_dev_len} tokens\") \n",
    "print(f\"Test: {max_test_len} tokens\")\n",
    "\n",
    "if max_train_len <= 448 and max_dev_len <= 448 and max_test_len <= 448:\n",
    "    print(\"âœ… All sequences are within Whisper's maximum length limit!\")\n",
    "else:\n",
    "    print(\"âŒ Warning: Some sequences still exceed maximum length!\")\n",
    "    \n",
    "print(\"ðŸŽ¯ Ready to start training...\")\n",
    "\n",
    "# === NOW PROCEED WITH TRAINING ===\n",
    "print('model training dir:', OUTPUT_DIR)\n",
    "\n",
    "# train from scratch\n",
    "trainer.train()\n",
    "\n",
    "# # alternatively, you can continue training if a previous job was interrupted\n",
    "# trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244a087",
   "metadata": {},
   "source": [
    "## Run the training\n",
    "\n",
    "Note: tensorboard doesn't show properly in jupyter notebooks, use the tensorboard_server.py tool to host a tensorboard instance on Modal, using below model training dir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd5fb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training dir: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_small_2\n"
     ]
    }
   ],
   "source": [
    "print('model training dir:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "740407b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/600 13:31 < 24:16, 0.26 it/s, Epoch 0.90/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Lattescore</th>\n",
       "      <th>Input Tokens Seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.905179</td>\n",
       "      <td>0.935144</td>\n",
       "      <td>0.491070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.716700</td>\n",
       "      <td>1.536061</td>\n",
       "      <td>0.730074</td>\n",
       "      <td>0.296942</td>\n",
       "      <td>2.205882</td>\n",
       "      <td>144000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.303100</td>\n",
       "      <td>1.191576</td>\n",
       "      <td>0.620538</td>\n",
       "      <td>0.256268</td>\n",
       "      <td>6.617647</td>\n",
       "      <td>288000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.181800</td>\n",
       "      <td>1.030626</td>\n",
       "      <td>0.550887</td>\n",
       "      <td>0.239161</td>\n",
       "      <td>13.235294</td>\n",
       "      <td>432000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.020700</td>\n",
       "      <td>0.952905</td>\n",
       "      <td>0.495326</td>\n",
       "      <td>0.196870</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>576000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.9351\n",
      "Adjusted CER: 0.4911\n",
      "LATTEScore: 0.00%\n",
      "Un-adjusted WER: 1.3069\n",
      "Un-adjusted CER: 0.7007\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.7301\n",
      "Adjusted CER: 0.2969\n",
      "LATTEScore: 2.21%\n",
      "Un-adjusted WER: 0.8456\n",
      "Un-adjusted CER: 0.3588\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.6205\n",
      "Adjusted CER: 0.2563\n",
      "LATTEScore: 6.62%\n",
      "Un-adjusted WER: 0.7480\n",
      "Un-adjusted CER: 0.3114\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.5509\n",
      "Adjusted CER: 0.2392\n",
      "LATTEScore: 13.24%\n",
      "Un-adjusted WER: 0.6375\n",
      "Un-adjusted CER: 0.2974\n",
      "===============\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4953\n",
      "Adjusted CER: 0.1969\n",
      "LATTEScore: 18.75%\n",
      "Un-adjusted WER: 0.5811\n",
      "Un-adjusted CER: 0.2380\n",
      "===============\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Labels' sequence length 616 cannot exceed the maximum allowed length of 448 tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train from scratch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# # alternatively, you can continue training if a previous job was interrupted\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#trainer.train(resume_from_checkpoint = True)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:3810\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3808\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3809\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3811\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3812\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1686\u001b[39m, in \u001b[36mWhisperForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels.shape[\u001b[32m1\u001b[39m] > \u001b[38;5;28mself\u001b[39m.max_target_positions:\n\u001b[32m-> \u001b[39m\u001b[32m1686\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1687\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabels\u001b[39m\u001b[33m'\u001b[39m\u001b[33m sequence length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot exceed the maximum allowed length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.max_target_positions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1688\u001b[39m         )\n\u001b[32m   1689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1690\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1691\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1692\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Labels' sequence length 616 cannot exceed the maximum allowed length of 448 tokens."
     ]
    }
   ],
   "source": [
    "# train from scratch\n",
    "trainer.train()\n",
    "\n",
    "# # alternatively, you can continue training if a previous job was interrupted\n",
    "#trainer.train(resume_from_checkpoint = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8842a",
   "metadata": {},
   "source": [
    "## Post-Training Evaluation\n",
    "\n",
    "when you run this after your training has finished it will use the best checkpoint (because we set \"load_best_model_at_end=True\" in the trainer args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2780af0",
   "metadata": {},
   "source": [
    "### On DEV set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eeecbc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4253\n",
      "Adjusted CER: 0.1928\n",
      "LATTEScore: 31.99%\n",
      "Un-adjusted WER: 0.5730\n",
      "Un-adjusted CER: 0.2603\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8349847197532654,\n",
       " 'eval_wer': 0.42534073607522993,\n",
       " 'eval_cer': 0.19279478039122483,\n",
       " 'eval_lattescore': 31.985294117647058,\n",
       " 'eval_runtime': 117.5694,\n",
       " 'eval_samples_per_second': 2.314,\n",
       " 'eval_steps_per_second': 0.289,\n",
       " 'epoch': 2.5210084033613445,\n",
       " 'num_input_tokens_seen': 1727520000}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (should give the same result shown in trainig progress on dev set)\n",
    "trainer.evaluate(dev_dataset, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d05701",
   "metadata": {},
   "source": [
    "### On TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e55c38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.3410\n",
      "Adjusted CER: 0.1362\n",
      "LATTEScore: 39.35%\n",
      "Un-adjusted WER: 0.3800\n",
      "Un-adjusted CER: 0.1564\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.806226909160614,\n",
       " 'eval_wer': 0.34104764837601076,\n",
       " 'eval_cer': 0.13624787270573063,\n",
       " 'eval_lattescore': 39.35018050541516,\n",
       " 'eval_runtime': 257.316,\n",
       " 'eval_samples_per_second': 2.153,\n",
       " 'eval_steps_per_second': 0.272,\n",
       " 'epoch': 2.5210084033613445,\n",
       " 'num_input_tokens_seen': 1727520000}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run on dev-set \n",
    "# (should give the same result shown in trainig progress on dev set)\n",
    "trainer.evaluate(test_dataset, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d1cc2f8-412d-4e50-8690-1da2893ea235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker metadata loaded: (52, 6)\n",
      "Columns: ['speaker_id', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "  speaker_id  gender    age  \\\n",
      "0     KES001  Female  30-40   \n",
      "1     KES002  Female  30-40   \n",
      "2     KES003    Male  25-30   \n",
      "3     KES004    Male  25-30   \n",
      "4     KES005    Male  18-24   \n",
      "\n",
      "                          severity_speech_impairment  \\\n",
      "0                       Severe (frequent breakdowns)   \n",
      "1                       Severe (frequent breakdowns)   \n",
      "2  Profound (communication very difficult or impo...   \n",
      "3                       Severe (frequent breakdowns)   \n",
      "4           Moderate (requires effort to understand)   \n",
      "\n",
      "             type_nonstandard_speech               etiology  \n",
      "0                         Dysarthria         Cerebral Palsy  \n",
      "1                         Dysarthria         Cerebral Palsy  \n",
      "2  Stuttering (Disfluency Disorders)         Cerebral Palsy  \n",
      "3  Stuttering (Disfluency Disorders)  Neurological disorder  \n",
      "4  Stuttering (Disfluency Disorders)  Neurological disorder  \n",
      "Generating predictions...\n",
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.4253\n",
      "Adjusted CER: 0.1928\n",
      "LATTEScore: 31.99%\n",
      "Un-adjusted WER: 0.5730\n",
      "Un-adjusted CER: 0.2603\n",
      "===============\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers not available, using WER-based LATTEScore approximation\n",
      "=== Metrics ===\n",
      "Adjusted WER: 0.3410\n",
      "Adjusted CER: 0.1362\n",
      "LATTEScore: 39.35%\n",
      "Un-adjusted WER: 0.3800\n",
      "Un-adjusted CER: 0.1564\n",
      "===============\n",
      "Dev WER: 0.698 | Word Accuracy: 30.2% | LATTEScore: 14.0%\n",
      "Test WER: 0.460 | Word Accuracy: 54.0% | LATTEScore: 31.4%\n",
      "\n",
      "Dev DataFrame shape: (272, 11)\n",
      "Columns: ['speaker_id', 'reference', 'prediction', 'wer', 'word_accuracy', 'lattescore_meaning_preserved', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "\n",
      "Test DataFrame shape: (554, 11)\n",
      "Columns: ['speaker_id', 'reference', 'prediction', 'wer', 'word_accuracy', 'lattescore_meaning_preserved', 'gender', 'age', 'severity_speech_impairment', 'type_nonstandard_speech', 'etiology']\n",
      "\n",
      "=== Model Deployment Analysis ===\n",
      "LATTEScore: 31.4%\n",
      "Deployment Threshold: 80.0%\n",
      "âŒ RECOMMENDATION: Model does not meet quality standards\n",
      "   Consider: More training data, hyperparameter tuning, or different architecture\n",
      "\n",
      "=== FILES SAVED ===\n",
      "dev_predictions.csv: 272 samples\n",
      "test_predictions.csv: 554 samples\n",
      "\n",
      "=== DATA PREVIEW ===\n",
      "  speaker_id                                          reference  \\\n",
      "0     KES004  kila disemba familia yetu hukutana kijijini kw...   \n",
      "1     KES004  katika shule ya msingi ya langas ni ni ni nili...   \n",
      "2     KES004  nilipofanya safari ya kitalii katika mbuga ya ...   \n",
      "3     KES004  kila jumapili mama huamka mapena na kutengenez...   \n",
      "4     KES004  nilipokua chuo kikuu nairobi tulikua na na uta...   \n",
      "5     KES004  katika kampaini za usafi mtaani kibera sisi ka...   \n",
      "6     KES004  nilipokuwa soko la gikomba nilikutana na mama ...   \n",
      "7     KES004  siku moja nilisaidia shangazi yangu kuvuna mah...   \n",
      "8     KES004  nilihudhuria harusi ya ndugu yangu katika kaun...   \n",
      "9     KES004  katika safari yangu ya ya kwenda kakamega tuli...   \n",
      "\n",
      "                                          prediction       wer  \\\n",
      "0  Kila Disemba familia yetu hukutana kijijini kw...  0.392857   \n",
      "1  Katika shule ya msingi ya lililanga nilijifunz...  0.243243   \n",
      "2  Nilipofanya safari yaitalika katika mbuga ya M...  0.409091   \n",
      "3  Kila jumapili mama wamka mapema na kutengeneza...  0.260870   \n",
      "4  Nilipokua chukuu Nairobi, tulikwana na utamadu...  0.555556   \n",
      "5  Katika kampeni za usafi mtaani kibera sisi kam...  0.208333   \n",
      "6  Nilipokuwa soko la Gikomba nilikukutana na mam...  0.466667   \n",
      "7  Siku mmoja nilisaidia shangazi yangu kuvuna ma...  0.187500   \n",
      "8  Nilihudhuria harusi ya ndugu yangu katika coun...  0.218750   \n",
      "9  Katika safari yangu ya ya kwenda kamega tulisi...  0.187500   \n",
      "\n",
      "   lattescore_meaning_preserved  \n",
      "0                             0  \n",
      "1                             1  \n",
      "2                             0  \n",
      "3                             1  \n",
      "4                             0  \n",
      "5                             1  \n",
      "6                             0  \n",
      "7                             1  \n",
      "8                             1  \n",
      "9                             1  \n",
      "\n",
      "=== DOWNLOAD LINKS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='dev_predictions.csv' target='_blank'>dev_predictions.csv</a><br>"
      ],
      "text/plain": [
       "/nairobo_innovation_sprint/dev_predictions.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='test_predictions.csv' target='_blank'>test_predictions.csv</a><br>"
      ],
      "text/plain": [
       "/nairobo_innovation_sprint/test_predictions.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEXT STEPS ===\n",
      "1. Analyze LATTEScore by speaker metadata (etiology, severity, gender)\n",
      "2. Compare LATTEScore with WER to see if meaning preservation differs from word accuracy\n",
      "3. Use LATTEScore for model deployment decisions\n",
      "4. Calculate LATTEScore breakdown by speaker characteristics\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import FileLink, display\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Load external speaker metadata directly ---\n",
    "metadata_url = \"https://huggingface.co/datasets/cdli/kenyan_english_nonstandard_speech_v0.9/resolve/main/speaker_metadata.tsv\"\n",
    "speaker_metadata_ds = load_dataset(\"csv\", data_files=metadata_url, sep=\"\\t\")[\"train\"]\n",
    "speaker_metadata = pd.DataFrame(speaker_metadata_ds)\n",
    "\n",
    "# Drop unwanted columns\n",
    "speaker_metadata = speaker_metadata.drop(columns=[\"comments\", \"slp_id\"], errors=\"ignore\")\n",
    "\n",
    "print(\"Speaker metadata loaded:\", speaker_metadata.shape)\n",
    "print(\"Columns:\", speaker_metadata.columns.tolist())\n",
    "print(speaker_metadata.head())\n",
    "\n",
    "# --- Function to calculate individual WER/accuracy ---\n",
    "def calculate_individual_wer(prediction, reference):\n",
    "    return wer_metric.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "# --- LATTEScore calculation function ---\n",
    "def calculate_lattescore(prediction, reference, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Calculate whether meaning is preserved for a single transcript pair\n",
    "    Returns 1 if meaning preserved, 0 if meaning lost\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Skip empty strings\n",
    "        if not reference.strip() or not prediction.strip():\n",
    "            return 0\n",
    "            \n",
    "        # Get sentence embeddings\n",
    "        emb_ref = model.encode(reference, convert_to_tensor=True)\n",
    "        emb_pred = model.encode(prediction, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(emb_ref, emb_pred).item()\n",
    "        \n",
    "        # Consider meaning preserved if similarity > threshold\n",
    "        return 1 if similarity > similarity_threshold else 0\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: use WER-based approximation\n",
    "        wer = wer_metric.compute(predictions=[prediction], references=[reference])\n",
    "        # Conservative threshold: meaning preserved if WER < 0.3 (30%)\n",
    "        return 1 if wer < 0.3 else 0\n",
    "\n",
    "# --- You need to get predictions first! Add this: ---\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Generate predictions for dev set\n",
    "preds_dev = trainer.predict(dev_dataset)\n",
    "dev_predictions = processor.tokenizer.batch_decode(preds_dev.predictions, skip_special_tokens=True)\n",
    "dev_references = [transcript_normalizer(x) for x in dev_dataset[\"transcription\"]]\n",
    "\n",
    "# Generate predictions for test set  \n",
    "preds_test = trainer.predict(test_dataset)\n",
    "test_predictions = processor.tokenizer.batch_decode(preds_test.predictions, skip_special_tokens=True)\n",
    "test_references = [transcript_normalizer(x) for x in test_dataset[\"transcription\"]]\n",
    "\n",
    "# Create prediction dictionaries\n",
    "preds_dev_dict = {\n",
    "    \"speaker_id\": dev_dataset[\"speaker_id\"],\n",
    "    \"transcription\": dev_references,\n",
    "    \"prediction\": dev_predictions\n",
    "}\n",
    "\n",
    "preds_test_dict = {\n",
    "    \"speaker_id\": test_dataset[\"speaker_id\"], \n",
    "    \"transcription\": test_references,\n",
    "    \"prediction\": test_predictions\n",
    "}\n",
    "\n",
    "# Calculate WER for entire sets\n",
    "wer_dev = wer_metric.compute(predictions=dev_predictions, references=dev_references)\n",
    "wer_test = wer_metric.compute(predictions=test_predictions, references=test_references)\n",
    "\n",
    "# Calculate metrics for individual examples\n",
    "dev_wer_individual = [calculate_individual_wer(p, r) for p, r in zip(dev_predictions, dev_references)]\n",
    "dev_acc_individual = [(1 - wer) * 100 for wer in dev_wer_individual]\n",
    "dev_lattescore_individual = [calculate_lattescore(p, r) for p, r in zip(dev_predictions, dev_references)]\n",
    "\n",
    "test_wer_individual = [calculate_individual_wer(p, r) for p, r in zip(test_predictions, test_references)]\n",
    "test_acc_individual = [(1 - wer) * 100 for wer in test_wer_individual]\n",
    "test_lattescore_individual = [calculate_lattescore(p, r) for p, r in zip(test_predictions, test_references)]\n",
    "\n",
    "# Calculate overall LATTEScore percentages\n",
    "dev_lattescore_percent = (sum(dev_lattescore_individual) / len(dev_lattescore_individual)) * 100\n",
    "test_lattescore_percent = (sum(test_lattescore_individual) / len(test_lattescore_individual)) * 100\n",
    "\n",
    "acc_dev = (1 - wer_dev) * 100\n",
    "acc_test = (1 - wer_test) * 100\n",
    "\n",
    "print(f\"Dev WER: {wer_dev:.3f} | Word Accuracy: {acc_dev:.1f}% | LATTEScore: {dev_lattescore_percent:.1f}%\")\n",
    "print(f\"Test WER: {wer_test:.3f} | Word Accuracy: {acc_test:.1f}% | LATTEScore: {test_lattescore_percent:.1f}%\")\n",
    "\n",
    "# --- Enhanced DataFrame creation with metadata merge ---\n",
    "def create_enhanced_dataframe(dataset, wer_individual, acc_individual, lattescore_individual, dataset_name):\n",
    "    base_data = {\n",
    "        \"speaker_id\": dataset[\"speaker_id\"],\n",
    "        \"reference\": dataset[\"transcription\"],\n",
    "        \"prediction\": dataset[\"prediction\"],\n",
    "        \"wer\": wer_individual,\n",
    "        \"word_accuracy\": acc_individual,\n",
    "        \"lattescore_meaning_preserved\": lattescore_individual,  # 1=preserved, 0=lost\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(base_data)\n",
    "    df = df.merge(speaker_metadata, on=\"speaker_id\", how=\"left\")\n",
    "\n",
    "    print(f\"\\n{dataset_name} DataFrame shape: {df.shape}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    return df\n",
    "\n",
    "# Build enhanced DataFrames (THIS WAS MISSING!)\n",
    "df_dev_enhanced = create_enhanced_dataframe(preds_dev_dict, dev_wer_individual, dev_acc_individual, dev_lattescore_individual, \"Dev\")\n",
    "df_test_enhanced = create_enhanced_dataframe(preds_test_dict, test_wer_individual, test_acc_individual, test_lattescore_individual, \"Test\")\n",
    "\n",
    "# --- Model deployment analysis using LATTEScore ---\n",
    "def analyze_model_deployment(lattescore, threshold=80.0):\n",
    "    \"\"\"Analyze if model meets quality standards based on LATTEScore\"\"\"\n",
    "    print(f\"\\n=== Model Deployment Analysis ===\")\n",
    "    print(f\"LATTEScore: {lattescore:.1f}%\")\n",
    "    print(f\"Deployment Threshold: {threshold}%\")\n",
    "    \n",
    "    if lattescore >= threshold:\n",
    "        print(\"âœ… RECOMMENDATION: Model meets quality standards for deployment\")\n",
    "        print(\"   The ASR model preserves meaning in most transcripts\")\n",
    "    else:\n",
    "        print(\"âŒ RECOMMENDATION: Model does not meet quality standards\")\n",
    "        print(\"   Consider: More training data, hyperparameter tuning, or different architecture\")\n",
    "    \n",
    "    return lattescore >= threshold\n",
    "\n",
    "# Run deployment analysis\n",
    "deployment_ready = analyze_model_deployment(test_lattescore_percent)\n",
    "\n",
    "# --- Save only the main analysis files ---\n",
    "df_dev_enhanced.to_csv(\"dev_predictions.csv\", index=False)\n",
    "df_test_enhanced.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== FILES SAVED ===\")\n",
    "print(f\"dev_predictions.csv: {len(df_dev_enhanced)} samples\")\n",
    "print(f\"test_predictions.csv: {len(df_test_enhanced)} samples\")\n",
    "\n",
    "# --- Preview data ---\n",
    "print(\"\\n=== DATA PREVIEW ===\")\n",
    "print(df_dev_enhanced[['speaker_id', 'reference', 'prediction', 'wer', 'lattescore_meaning_preserved']].head(10))\n",
    "\n",
    "# --- Download links ---\n",
    "print(\"\\n=== DOWNLOAD LINKS ===\")\n",
    "display(FileLink(\"dev_predictions.csv\"))\n",
    "display(FileLink(\"test_predictions.csv\"))\n",
    "\n",
    "# --- Updated next steps ---\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Analyze LATTEScore by speaker metadata (etiology, severity, gender)\")\n",
    "print(\"2. Compare LATTEScore with WER to see if meaning preservation differs from word accuracy\")\n",
    "print(\"3. Use LATTEScore for model deployment decisions\")\n",
    "print(\"4. Calculate LATTEScore breakdown by speaker characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9082ef",
   "metadata": {},
   "source": [
    "## Store Model\n",
    "\n",
    "--> save best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d4454",
   "metadata": {},
   "source": [
    "### Save to your volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ff87bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: /jupyter_kernel/trained_models/sw_nonstandard_tune_whisper_small_2/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with \"load_best_model_at_end=True\" set in the settings (this is the default, so don't change that), after training is completed the best model is loaded and then saved\n",
    "best_model_dir = os.path.join(OUTPUT_DIR, 'best_model')\n",
    "print(f\"Saving to: {best_model_dir}\")\n",
    "trainer.model.save_pretrained(best_model_dir, safe_serialization=True)\n",
    "trainer.tokenizer.save_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81bd7a-f6ed-43f6-8a54-2d9bd5c55abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
